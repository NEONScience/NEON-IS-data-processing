##############################################################################################
#' @title Compute precipitation from collector depths reported by the Belfort AEPG 600m 
#' precipitation collector 

#' @author
#' Teresa Burlingame \email{tburlingame@battelleecology.org} \cr
#' Cove Sturtevant \email{csturtevant@battelleecology.org} \cr

#' @description Wrapper function. Aggregate and smooth strain gauge depth data reported by the 
#' Belfort AEPG 600m weighing gauge sensor, and compute precipitation. Uncertainty estimates
#' are produced via a Monte Carlo approach using IAAFT surrogate data. 

#' @param DirIn Character value. The input path to the data from a single source ID, structured as follows: 
#' #/pfs/BASE_REPO/#/yyyy/mm/dd/#/location-id, where # indicates any number of parent and child directories 
#' of any name, so long as they are not 'pfs' or recognizable as the 'yyyy/mm/dd' structure which indicates 
#' the 4-digit year, 2-digit month, and' 2-digit day. The location-id is the unique identifier of the location. \cr
#'
#' Nested within this path are (at a minimum) the folders:
#'         /data
#'         /flags
#'         /threshold
#' The data folder holds a multiple data files with strain gauge depth data, one file per day, 
#' from the same sensor/location. The daily files should be sequential such that they comprise a dataset
#' from the same sensor/location over a window of several days around 3 central processing days. The
#' window of time surrounding these days should be adequate such that edge effects in applying the 
#' smoothing algorithm are minimal for the central days. The data files are named in the convention:
#' SOURCETYPE_LOCATIONID_YYYY-MM-DD.parquet
#' 
#' The flags folder holds two files containing basic plausibility and calibration quality flags for 
#' the central processing day only. These files should respectively be named in the convention:
#' SOURCETYPE_LOCATIONID_YYYY-MM-DD_flagsPlausibility.parquet
#' SOURCETYPE_LOCATIONID_YYYY-MM-DD_flagsCal.parquet
#' All other files in this directory will be ignored.
#'
#' The threshold folder contains a single file named thresholds.json that holds threshold parameters
#' applicable to the smoothing algorithm. 
#'
#' @param DirOutBase Character value. The output path that will replace the #/pfs/BASE_REPO portion of DirIn. 
#'
#' @param SchmStatHour (Optional). A json-formatted character string containing the schema for the hourly 
#' output precipitation statistics and quality flags generated by this algorithm. May be NULL (default), 
#' in which case the variable names will be automatically determined.  
#' ENSURE THAT ANY PROVIDED OUTPUT SCHEMA MATCHES THE ORDER OF THE OUTPUT. Otherwise, outputs will be 
#' labeled incorrectly. Best practice is to run the code without any schema first to understand the ordering
#' of the outputs, then relabel them as desired by providing a schema.

#' @param SchmStatDay (Optional). A json-formatted character string containing the schema for the daily 
#' output precipitation statistics and quality flags. May be NULL (default), 
#' in which case the  the variable names will be automatically determined.  
#' ENSURE THAT ANY PROVIDED OUTPUT SCHEMA MATCHES THE ORDER OF THE OUTPUT.Otherwise, outputs will be 
#' labeled incorrectly. Best practice is to run the code without any schema first to understand the ordering
#' of the outputs, then relabel them as desired by providing a schema.

#' @param SchmQfGage (Optional). A json-formatted character string containing the schema for the aggregated 
#' strain gauge flags. May be NULL (default), 
#' in which case the  the variable names will be automatically determined.  
#' ENSURE THAT ANY PROVIDED OUTPUT SCHEMA MATCHES THE ORDER OF THE OUTPUT. Otherwise, outputs will be 
#' labeled incorrectly. Best practice is to run the code without any schema first to understand the ordering
#' of the outputs, then relabel them as desired by providing a schema.

#' @param DirSubCopy (optional) Character vector. The names of additional subfolders at 
#' the same level as the data/flags/threshold folders in the input path that are to be copied with a 
#' symbolic link to the output path (i.e. carried through as-is). Note that the 'stats' and 'flags' directories 
#' are automatically populated in the output and cannot be included here.

#' @param log A logger object as produced by NEONprocIS.base::def.log.init to produce structured log
#' output. Defaults to NULL, in which the logger will be created and used within the function. See NEONprocIS.base::def.log.init
#' for more details.
#'
#' @return A repository in DirOutBase containing the precipitation and uncertainty estimates along with 
#' quality flags (incl. the final quality flag) produced by the depth smoothing algorithm, where DirOutBase 
#' replaces BASE_REPO of argument \code{DirIn} but otherwise retains the child directory structure of the 
#' input path. The terminal directories for each datum include, at a minimum, 'stats' and 'flags'. The stats folder
#' contains hourly and daily output for 3 data days centered on the day indicated in the path structure, one 
#' file per day and output frequency (i.e. 6 total files), named appropriately. The output in the stats folder 
#' contains the precipitation sum, uncertainty estimates, quality flags specific to the smoothing algorithm, 
#' and final quality flag. The 'flags' folder will contain the plausibility and calibration flags aggregated
#' across the 3 strain gauges at the original measurement frequency for later processing into quality metrics.
#'
#' @references
#' License: (example) GNU AFFERO GENERAL PUBLIC LICENSE Version 3, 19 November 2007

#' @keywords Currently none

#' @examples 
#' # NOT RUN
#' DirIn <- '/scratch/pfs/precipWeighing_thresh_select_ts_pad/2024/03/02/precip-weighing_ARIK900000'
#' DirOutBase <- '/scratch/pfs/out'
#' wrap.precip.aepg.smooth(DirIn,DirOutBase,FileSchmL0)

#' @seealso Currently none

# changelog and author contributions / copyrights
#   Teresa Burlingame & Cove Sturtevant (2024-06-25)
#     Initial creation
##############################################################################################
wrap.precip.aepg.smooth <- function(DirIn,
                                    DirOutBase,
                                    SchmStatHour=NULL,
                                    SchmStatDay=NULL,
                                    SchmQfGage=NULL,
                                    DirSubCopy=NULL,
                                    log=NULL
){
  
  # Start logging if not already
  if(base::is.null(log)){
    log <- NEONprocIS.base::def.log.init()
  } 
  

  # Gather info about the input directory and create the output directory.
  InfoDirIn <- NEONprocIS.base::def.dir.splt.pach.time(DirIn,log=log)
  dirInData <- fs::path(DirIn,'data')
  dirInQf <- fs::path(DirIn,'flags')
  dirThsh <- base::paste0(DirIn,'/threshold')
  fileThsh <- base::dir(dirThsh)
  dirOut <- fs::path(DirOutBase,InfoDirIn$dirRepo)
  dirOutQf <- fs::path(dirOut,'flags')
  dirOutStat <- fs::path(dirOut,'stats')
  NEONprocIS.base::def.dir.crea(DirBgn = dirOut,
                                DirSub = c('stats','flags'),
                                log = log)
  
  # Copy with a symbolic link the desired subfolders 
  DirSubCopy <- base::unique(base::setdiff(DirSubCopy,c('stats','flags')))
  if(base::length(DirSubCopy) > 0){

    NEONprocIS.base::def.dir.copy.symb(DirSrc=fs::path(DirIn,DirSubCopy),
                                       DirDest=dirOut,
                                       LnkSubObj=FALSE,
                                       log=log)
  }    
  
  # Read in the thresholds file (read first file only, there should only be 1)
  if(base::length(fileThsh) > 1){
    fileThsh <- fileThsh[1]
    log$info(base::paste0('There is more than one threshold file in ',dirThsh,'. Using ',fileThsh))
  }
  thsh <- NEONprocIS.qaqc::def.read.thsh.qaqc.df((NameFile=base::paste0(dirThsh,'/',fileThsh)))
  
  # Verify that the term(s) needed in the input parameters are included in the threshold files
  termTest <- "precipBulk"
  exstThsh <- termTest %in% base::unique(thsh$term_name) # Do the terms exist in the thresholds
  if(base::sum(exstThsh) != base::length(termTest)){
    log$error(base::paste0('Thresholds for term(s): ',base::paste(termTest[!exstThsh],collapse=','),' do not exist in the thresholds file. Cannot proceed.')) 
    stop()
  }
  # Assign thresholds
  thshIdxTerm <- thsh[thsh$term_name == termTest,]
  WndwAgr = thshIdxTerm$string_value[thshIdxTerm$threshold_name == 'WndwAgr']
  RangeSizeHour = thshIdxTerm$number_value[thshIdxTerm$threshold_name == 'RangeSizeHour']
  Envelope = thshIdxTerm$number_value[thshIdxTerm$threshold_name == 'Envelope']
  ThshCountHour = thshIdxTerm$number_value[thshIdxTerm$threshold_name == 'ThshCountHour']
  Quant = thshIdxTerm$number_value[thshIdxTerm$threshold_name == 'Quant']
  ThshChange = thshIdxTerm$number_value[thshIdxTerm$threshold_name == 'ThshChange']
  ChangeFactor = thshIdxTerm$number_value[thshIdxTerm$threshold_name == 'ChangeFactor']
  ChangeFactorEvap = thshIdxTerm$number_value[thshIdxTerm$threshold_name == 'ChangeFactorEvap']
  Recharge = thshIdxTerm$number_value[thshIdxTerm$threshold_name == 'Recharge']
  ExtremePrecipMax = thshIdxTerm$number_value[thshIdxTerm$threshold_name == 'ExtremePrecipMax']

  # Adjust thresholds based on WndwAgr unit
  WndwAgrNumc <- as.numeric(stringr::str_extract(string = WndwAgr, pattern = '[0-9]+'))
  if(stringr::str_detect(WndwAgr, 'min')) {
    ThshCount <- ThshCountHour * (60/WndwAgrNumc) 
    rangeSize <- RangeSizeHour*(60/WndwAgrNumc)   
    
    if(WndwAgrNumc > 60 | WndwAgrNumc < 5){
      log$error('averaging unit must be between 5 minutes and one hour')
      stop()
    }
  } else if ((stringr::str_detect(WndwAgr, 'hour')) ){
    ThshCount <- ThshCountHour/WndwAgrNumc
    rangeSize <- RangeSizeHour/WndwAgrNumc
    
    if(WndwAgrNumc > 1 | WndwAgrNumc < (5/60)){
      log$error('averaging unit must be between 5 minutes and one hour')
      stop()
    }
  } else {
    log$error('averaging unit needs to be in minutes (min) or hours (hour)')
    stop()
  }
  
  # Take stock of our data files.
  fileData <- base::list.files(dirInData,pattern='.parquet',full.names=FALSE)
  fileQfPlau <- base::list.files(dirInQf,pattern='Plausibility.parquet',full.names=FALSE)
  fileQfCal <- base::list.files(dirInQf,pattern='Cal.parquet',full.names=FALSE)
  
  # Read the datasets 
  data <- NEONprocIS.base::def.read.parq.ds(fileIn=fs::path(dirInData,fileData),
                                            VarTime='readout_time',
                                            RmvDupl=TRUE,
                                            Df=TRUE, 
                                            log=log)
  
  qfPlau <- NEONprocIS.base::def.read.parq.ds(fileIn=fs::path(dirInQf,fileQfPlau),
                                                 VarTime='readout_time',
                                                 RmvDupl=TRUE,
                                                 Df=TRUE,
                                                 log=log)
  
  qfCal <- NEONprocIS.base::def.read.parq.ds(fileIn=fs::path(dirInQf,fileQfCal),
                                                VarTime='readout_time',
                                                RmvDupl=TRUE,
                                                Df=TRUE,
                                                log=log)
  
  qf <- dplyr::full_join(qfPlau, qfCal, by =  'readout_time')
  
  
  #combine three gauges into one flagging variable. If any are 1 all flagged, any -1 all flagged, else not flagged
  qfNames <- names(qf)[grepl(unique(names(qf)), pattern = 'strainGauge')]
  qfNames <- unique(sub(pattern='strainGauge[1-3]Depth',replacement='',x=qfNames))
  qfs<- qf[, 'readout_time', drop = F]
  for (name in qfNames){
    qf_sub <- qf[,grepl(names(qf), pattern = name)]
    qfVar <- paste0('strainGaugeDepth', name)
    qfs[[qfVar]] <- NA
    qf_0 <- rowSums(qf_sub == 0, na.rm = T)
    qfs[[qfVar]][qf_0 == ncol(qf_sub)] <- 0
    qf_1 <- rowSums(qf_sub == 1, na.rm = T)
    qfs[[qfVar]][qf_1 >=1] <- 1
    qf_neg1 <- rowSums(qf_sub == -1, na.rm = T)
    qfs[[qfVar]][is.na(qfs[[qfVar]]) & qf_neg1 >=1] <- -1
    qfs[[qfVar]][is.na(qfs[[qfVar]])] <- -1
  }
  
  # Aggregate depth streams into a single depth, and remove values where not all three are available/good
  data$strain_gauge1_stability[is.na(data$strainGauge1Depth)] <- as.numeric(NA)
  data$strain_gauge2_stability[is.na(data$strainGauge2Depth)] <- as.numeric(NA)
  data$strain_gauge3_stability[is.na(data$strainGauge3Depth)] <- as.numeric(NA)
  data <- data %>% dplyr::mutate(strainGaugeDepth = base::rowMeans(x=base::cbind(strainGauge1Depth, 
                                                                                 strainGauge2Depth, 
                                                                                 strainGauge3Depth), na.rm = F),
                                 strainGaugeStability = base::rowSums(x=base::cbind(strain_gauge1_stability, 
                                                                                    strain_gauge2_stability, 
                                                                                    strain_gauge3_stability), na.rm = F)==3)
  data$strainGaugeDepth[data$strainGaugeStability == FALSE] <- as.numeric(NA)
  
  # if there are no heater streams add them in as NA
  if(!('internal_temperature' %in% names(data))){data$internal_temperature <- as.numeric(NA)}
  if(!('inlet_temperature' %in% names(data))){data$inlet_temperature <- as.numeric(NA)}
  if(!('orifice_heater_flag' %in% names(data))){data$orifice_heater_flag <- as.numeric(NA)}
  
  # Add the suspectCal flag to the data so that it can be time-averaged and fed into the final QF
  data$strainGaugeDepthSuspectCalQF <- qfs$strainGaugeDepthSuspectCalQF
  
  # Do time averaging
  strainGaugeDepthAgr <- data %>%
    dplyr::mutate(startDateTime = lubridate::floor_date(as.POSIXct(readout_time, tz = 'UTC'), unit = WndwAgr)) %>%
    dplyr::mutate(endDateTime = lubridate::ceiling_date(as.POSIXct(readout_time, tz = 'UTC'), unit = WndwAgr,change_on_boundary=TRUE)) %>%
    dplyr::group_by(startDateTime,endDateTime) %>%
    dplyr::summarise(strainGaugeDepth = mean(strainGaugeDepth, na.rm = T),
                     strainGaugeStability = dplyr::if_else(all(is.na(strainGaugeStability)),as.numeric(NA),all(strainGaugeStability==TRUE, na.rm = T)),
                     inletTemperature = mean(inlet_temperature, na.rm = T),
                     internalTemperature = mean(internal_temperature, na.rm = T), 
                     orificeHeaterFlag = max(orifice_heater_flag, na.rm = T), #used to see if heater was on when temps were above heating threshold (heaterErrorQF)
                     inletHeater1QM = round((length(which(orifice_heater_flag == 100))/dplyr::n())*100,1),
                     inletHeater2QM = round((length(which(orifice_heater_flag == 110))/dplyr::n())*100,1), 
                     inletHeater3QM = round((length(which(orifice_heater_flag == 111))/dplyr::n())*100,1),
                     inletHeaterNAQM = round((length(which(is.na(orifice_heater_flag)))/dplyr::n())*100,1),
                     suspectCalQF = max(strainGaugeDepthSuspectCalQF,na.rm = T)) 
  
  # Initialize time-aggregated flags
  qfAgr <- strainGaugeDepthAgr %>% dplyr::select(startDateTime, endDateTime)
  qfAgr$insuffDataQF <- 0
  qfAgr$extremePrecipQF <- 0
  qfAgr$dielNoiseQF <- 0
  qfAgr$strainGaugeStabilityQF <- 0
  qfAgr$strainGaugeStabilityQF[is.na(strainGaugeDepthAgr$strainGaugeStability)] <- -1 
  qfAgr$strainGaugeStabilityQF[strainGaugeDepthAgr$strainGaugeStability == FALSE] <- 1 
  qfAgr$heaterErrorQF <- 0
  qfAgr$evapDetectedQF <- 0
  qfAgr$heaterErrorQF[strainGaugeDepthAgr$internalTemperature > -6 & 
                           strainGaugeDepthAgr$internalTemperature < 2 & 
                           strainGaugeDepthAgr$inletTemperature < strainGaugeDepthAgr$internalTemperature] <- 1
  qfAgr$heaterErrorQF[strainGaugeDepthAgr$internalTemperature > 6 & strainGaugeDepthAgr$orificeHeaterFlag > 0] <- 1
  qfAgr$heaterErrorQF[is.na(strainGaugeDepthAgr$internalTemperature) | 
                         is.na(strainGaugeDepthAgr$inletTemperature) | 
                         is.na(strainGaugeDepthAgr$orificeHeaterFlag)] <- -1
  
  
  

  # Dynamic Envelope
  # Identify no-rain days in order to apply a dynamic envelope calculation
  # Require that there be no change in depth between the start and end of the day that is
  # greater than a percentage of the pre-defined envelope set in the thresholds
  dataHourly <- strainGaugeDepthAgr %>%
    dplyr::mutate(startDateTime = lubridate::floor_date(startDateTime, unit = 'hour')) %>%
    dplyr::group_by(startDateTime) %>%
    dplyr::summarise(strainGaugeDepthMed = median(strainGaugeDepth, na.rm = T),
                     strainGaugeDepthMin = min(strainGaugeDepth, na.rm = T), 
                     strainGaugeDepthMax = max(strainGaugeDepth, na.rm = T))
  dataDaily <- dataHourly %>%
    dplyr::mutate(startDateTime = lubridate::floor_date(startDateTime, unit = 'day')) %>%
    dplyr::group_by(startDateTime) %>%
    dplyr::summarise(strainGaugeDepthChg = tail(strainGaugeDepthMax,1)-head(strainGaugeDepthMin,1))
  setNoRain <- (dataDaily$strainGaugeDepthChg < 0.25*Envelope) & (dataDaily$strainGaugeDepthChg > -1*Envelope)
  setNoRain[is.na(setNoRain)] <- FALSE
  
  # Recompute the envelope if we have determined days without rain
  if(any(setNoRain)){
    dayNoRain <- dataDaily$startDateTime[setNoRain]
    timeDay <- lubridate::floor_date(strainGaugeDepthAgr$startDateTime,unit='day')
    
    envelopeComp <- data.frame(day=unique(timeDay),envelope=as.numeric(NA))
    
    for (idxDay in unique(timeDay)){
      if(!(idxDay %in% dayNoRain)){
        next
      }
      setDay <- timeDay == idxDay # row indices for this day
      envelopeIdx <- max(strainGaugeDepthAgr$strainGaugeDepth[setDay],na.rm=TRUE)-min(strainGaugeDepthAgr$strainGaugeDepth[setDay],na.rm=TRUE)
      envelopeComp$envelope[envelopeComp$day == idxDay] <- envelopeIdx
    }
    
    # Take the max envelope
    envelopeMax<- max(envelopeComp$envelope,na.rm=TRUE)
    if(!is.na(envelopeMax)){
      Envelope <- envelopeMax
    }
    
    # if Envelope is larger than Recharge threshold adjust recharge. 
    if(Envelope > Recharge/3){
      Recharge <- 3*Envelope
    }
  }

  #initialize fields
  strainGaugeDepthAgr$bench <- as.numeric(NA)
  strainGaugeDepthAgr$precip <- FALSE # TRUE when rain detected
  strainGaugeDepthAgr$precipType <- as.character(NA)

  # Initialize & pre-allocate surrogate data for uncertainty analysis
  numRow <- nrow(strainGaugeDepthAgr)
  nSurr <- 30
  surr <- matrix(as.numeric(NA),nrow=numRow,ncol=nSurr)
  nameVarDepthS <- paste0('strainGaugeDepthS',seq_len(nSurr))
  nameVarBenchS <- paste0('benchS',seq_len(nSurr))
  nameVarPrecipS <- paste0('precipS',seq_len(nSurr))
  nameVarPrecipTypeS <- paste0('precipTypeS',seq_len(nSurr))
  nameVarPrecipBulkS <- paste0('precipBulkS',seq_len(nSurr))
  strainGaugeDepthAgr[,nameVarDepthS] <- as.numeric(NA)
  strainGaugeDepthAgr[,nameVarBenchS] <- as.numeric(NA)
  strainGaugeDepthAgr[,c(nameVarPrecipS)] <- FALSE
  strainGaugeDepthAgr[,nameVarPrecipTypeS] <- as.character(NA)
  strainGaugeDepthAgr[,nameVarPrecipBulkS] <- as.numeric(NA)

  for(idxSurr in c(0,seq_len(nSurr))){
    
    if (idxSurr == 0){
      log$debug(paste0('Running original timeseries for datum ',DirIn))
      nameVarDepth <- 'strainGaugeDepth'
      nameVarBench <- 'bench'
      nameVarPrecip <- 'precip'
      nameVarPrecipType <- 'precipType'
      nameVarPrecipBulk <- 'precipBulk'
      
      strainGaugeDepthS <- strainGaugeDepthAgr$strainGaugeDepth
      
    } else {
      
      log$debug(paste0('Running Surrogate ',idxSurr, ' for datum ',DirIn))
      nameVarDepth <- paste0('strainGaugeDepthS',idxSurr)
      nameVarBench <- paste0('benchS',idxSurr)
      nameVarPrecip <- paste0('precipS',idxSurr)
      nameVarPrecipType <- paste0('precipTypeS',idxSurr)
      nameVarPrecipBulk <- paste0('precipBulkS',idxSurr)
      
      # If this is the first surrogate, create them
      if(idxSurr == 1){
        depthMinusBench <- strainGaugeDepthAgr$strainGaugeDepth - strainGaugeDepthAgr$bench # remove the computed benchmark
        setNotNa <- !is.na(depthMinusBench) # Remove all NA

        # Remove rangeSize amount of data at beginning and end of timeseries, which can have untracked changes in benchmark that corrupt the surrogates
        setNotNa[1:rangeSize] <- FALSE 
        setNotNa[(numRow-rangeSize+1):numRow] <- FALSE
        
        # Create surrogates
        surrFill <- base::try(multifractal::iaaft(x=depthMinusBench[setNotNa],N=nSurr),silent=F)
        if("try-error" %in% base::class(surrFill)){
          log$warn('Surrogate generation failed (could be not enough data). Uncertainty estimates cannot be generated. A check will be done later to ensure precip is also NA. If not, an error will occur.')
          break
        }
        strainGaugeDepthAgr[setNotNa,nameVarDepthS] <- strainGaugeDepthAgr$bench[setNotNa] + surrFill    # Add the surrogates to the benchmark
        
        # Backfill rangeSize amount of data at beginning and end of timeseries to the original strain gauge depth to form a complete timeseries
        strainGaugeDepthAgr[1:rangeSize,nameVarDepthS] <- strainGaugeDepthAgr$strainGaugeDepth[1:rangeSize] 
        strainGaugeDepthAgr[(numRow-rangeSize+1):numRow,nameVarDepthS] <- strainGaugeDepthAgr$strainGaugeDepth[(numRow-rangeSize+1):numRow] 
      }
      
      strainGaugeDepthS <- strainGaugeDepthAgr[[nameVarDepth]]
      
    }  
    
    # Run the smoothing algorithm
    precipSmooth <- def.precip.depth.smooth(dateTime=strainGaugeDepthAgr$startDateTime,
                                            gaugeDepth=strainGaugeDepthS,
                                            RangeSize=rangeSize,
                                            Quant=Quant,
                                            ThshCount=ThshCount,
                                            Envelope=Envelope,
                                            ThshChange=ThshChange,
                                            ChangeFactor=ChangeFactor,
                                            ChangeFactorEvap=ChangeFactorEvap,
                                            Recharge=Recharge,
                                            log=log)

    # Reassign outputs
    strainGaugeDepthAgr[[nameVarBench]] <- precipSmooth$bench
    strainGaugeDepthAgr[[nameVarPrecip]] <- precipSmooth$precip 
    strainGaugeDepthAgr[[nameVarPrecipType]] <- precipSmooth$precipType
    if(idxSurr == 0){
      # Only save evapDetectedQF if we processed the original data
      qfAgr$evapDetectedQF <- precipSmooth$evapDetectedQF
    }
    
    # Compute precip
    strainGaugeDepthAgr[[nameVarPrecipBulk]] <- c(base::diff(strainGaugeDepthAgr[[nameVarBench]]),as.numeric(NA))
    strainGaugeDepthAgr[[nameVarPrecipBulk]][strainGaugeDepthAgr[[nameVarPrecipBulk]] < 0] <- 0
    strainGaugeDepthAgr[[nameVarPrecipType]] <- c(strainGaugeDepthAgr[[nameVarPrecipType]][2:numRow],as.numeric(NA)) # Shift precip type to align with precip
    
  } # End loop around surrogates
  
  # Compute the variability in computed benchmark of the surrogates (incl actual benchmark)
  # The uncertainty of a sum or difference is equal to their individual uncertainties added in quadrature.
  nameVar <- names(strainGaugeDepthAgr)
  nameVarBenchS <- c('bench',nameVar[grepl('benchS[0-9]',nameVar)])
  strainGaugeDepthAgr$benchS_std <- matrixStats::rowSds(as.matrix(strainGaugeDepthAgr[,nameVarBenchS]))

  # Post-precip computation 
  qfAgr$insuffDataQF[is.na(strainGaugeDepthAgr$precipBulk)] <- 1

  # Envelope == Massive --> Flag all the data
  if(all(qfAgr$insuffDataQF == 1)){
    qfAgr$dielNoiseQF <- -1
  } else if(Envelope > 10){
    qfAgr$dielNoiseQF <- 1
  }

  # Clean up flag logic for NA data
  qfAgr$evapDetectedQF[qfAgr$insuffDataQF == 1] <- -1
  qfAgr$extremePrecipQF[qfAgr$insuffDataQF == 1] <- -1
  
  # Join qfAgr into strainGaugeDepthAgr
  strainGaugeDepthAgr <- dplyr::full_join(strainGaugeDepthAgr, qfAgr, by = c('startDateTime', 'endDateTime'))
  
  # Aggregate to the hour
  statsAgrHour <- strainGaugeDepthAgr %>%
    mutate(startDateTime = lubridate::floor_date(startDateTime, '1 hour')) %>%
    mutate(endDateTime = lubridate::ceiling_date(endDateTime, '1 hour')) %>%
    group_by(startDateTime,endDateTime) %>%
    summarise(
      precipBulk = sum(precipBulk),
      insuffDataQF = max(insuffDataQF, na.rm = T),
      extremePrecipQF = max(extremePrecipQF, na.rm = T),
      heaterErrorQF = ifelse(all(is.na(heaterErrorQF)),
                             -1,
                             ifelse(sum(heaterErrorQF==1, na.rm=T) >= 0.5*dplyr::n(),
                                    1,
                                    ifelse(all(heaterErrorQF==-1),
                                           -1,
                                           0))),
      dielNoiseQF = max(dielNoiseQF, na.rm = T), # Just a placeholder. Computed below.
      strainGaugeStabilityQF = max(strainGaugeStabilityQF, na.rm = T),
      evapDetectedQF = max(evapDetectedQF, na.rm = T),
      inletHeater1QM = mean(inletHeater1QM, na.rm = T),
      inletHeater2QM = mean(inletHeater2QM, na.rm = T),
      inletHeater3QM = mean(inletHeater3QM, na.rm = T),
      inletHeaterNAQM = mean(inletHeaterNAQM, na.rm = T),
      suspectCalQF = max(suspectCalQF,na.rm = T)
    )
  statsAgrHour$suspectCalQF[!(statsAgrHour$suspectCalQF %in% c(-1,0,1))] <- -1
  
  # Flag for max precip over 60-min - based on hourly totals
  statsAgrHour$extremePrecipQF[statsAgrHour$precipBulk > ExtremePrecipMax] <- 1
  
  # Compute hourly final quality flag
  statsAgrHour$finalQF <- 0
  qf_sub <- statsAgrHour[,c('insuffDataQF','extremePrecipQF', 'heaterErrorQF','suspectCalQF')]
  qf_1 <- rowSums(qf_sub == 1, na.rm = T) 
  statsAgrHour$finalQF[qf_1 >=1] <- 1 
  
  
  # Aggregate to the day
  statsAgrDay <- statsAgrHour %>%
    mutate(date = lubridate::floor_date(startDateTime, '1 day')) %>%
    group_by(date) %>%
    summarise(precipBulk = sum(precipBulk),
              insuffDataQF = max(insuffDataQF, na.rm = T),
              extremePrecipQF = max(extremePrecipQF, na.rm = T),
              heaterErrorQF = ifelse(all(is.na(heaterErrorQF)),
                                     -1,
                                     ifelse(sum(heaterErrorQF==1, na.rm=T) >= 0.5*dplyr::n(),
                                            1,
                                            ifelse(all(heaterErrorQF==-1),
                                                   -1,
                                                   0))),
              dielNoiseQF = max(dielNoiseQF, na.rm = T), # Just a placeholder. Computed below.
              strainGaugeStabilityQF = max(strainGaugeStabilityQF, na.rm = T),
              evapDetectedQF = max(evapDetectedQF, na.rm = T),
              inletHeater1QM = mean(inletHeater1QM, na.rm = T),
              inletHeater2QM = mean(inletHeater2QM, na.rm = T),
              inletHeater3QM = mean(inletHeater3QM, na.rm = T),
              inletHeaterNAQM = mean(inletHeaterNAQM, na.rm = T),
              suspectCalQF = max(suspectCalQF,na.rm = T)
    )
  statsAgrDay$suspectCalQF[!(statsAgrDay$suspectCalQF %in% c(-1,0,1))] <- -1
  
  # Compute daily final QF
  statsAgrDay$finalQF <- 0
  qf_sub <- statsAgrDay[,c('insuffDataQF','extremePrecipQF', 'heaterErrorQF','suspectCalQF')]
  qf_1 <- rowSums(qf_sub == 1, na.rm = T) 
  statsAgrDay$finalQF[qf_1 >=1] <- 1 
  
  
  # Aggregate uncertainty to the hour and day
  # Report daily precip, and uncertainty for the central day
  # We can use the same equation here, adding the uncertainties for the start and
  # end of the day in quadrature, with the caveat that the benchmark does not drop 
  # over the course of the hour/day. If this occurs we need to compute for each leg of 
  # a flat or increasing benchmark, summing the legs in quadrature
  
  # Hourly
  hours <- seq.POSIXt(from=strainGaugeDepthAgr$startDateTime[1],to=strainGaugeDepthAgr$startDateTime[numRow],by='hour')
  ucrtAgrHour <- lapply(hours,FUN=function(hourIdx){
    setUcrt <- which(strainGaugeDepthAgr$startDateTime >= hourIdx & 
                       strainGaugeDepthAgr$startDateTime <= (hourIdx + as.difftime(1,units='hours'))) # include first point of next day, because that is the point from which the difference is taken
    ucrtAgr <- def.ucrt.agr.precip.bench(strainGaugeDepthAgr$bench[setUcrt],strainGaugeDepthAgr$benchS_std[setUcrt])
    return(ucrtAgr)
  })
  statsAgrHour$precipBulkExpUncert <- 2*unlist(ucrtAgrHour)
  statsAgrHour$precipBulkExpUncert[is.na(statsAgrHour$precipBulk)] <- as.numeric(NA) # last value will always be NA because we don't have the start of the next day
  statsAgrHour <- statsAgrHour[,c("startDateTime","endDateTime","precipBulk","precipBulkExpUncert","insuffDataQF",
                                  "extremePrecipQF","heaterErrorQF","dielNoiseQF","strainGaugeStabilityQF",
                                  "evapDetectedQF","inletHeater1QM","inletHeater2QM","inletHeater3QM",
                                  "inletHeaterNAQM","finalQF")]
  
  # Daily
  days <- seq.POSIXt(from=strainGaugeDepthAgr$startDateTime[1],to=strainGaugeDepthAgr$startDateTime[numRow],by='day')
  ucrtAgrDay <- lapply(days,FUN=function(dayIdx){
    setUcrt <- which(strainGaugeDepthAgr$startDateTime >= dayIdx & 
                       strainGaugeDepthAgr$startDateTime <= (dayIdx + as.difftime(1,units='days'))) # include first point of next day, because that is the point from which the difference is taken
    ucrtAgr <- def.ucrt.agr.precip.bench(strainGaugeDepthAgr$bench[setUcrt],strainGaugeDepthAgr$benchS_std[setUcrt])
    return(ucrtAgr)
  })
  statsAgrDay$precipBulkExpUncert <- 2*unlist(ucrtAgrDay)
  statsAgrDay$precipBulkExpUncert[is.na(statsAgrDay$precipBulk)] <- as.numeric(NA) # last value will always be NA because we don't have the start of the next day
  statsAgrDay <- statsAgrDay[,c("date","precipBulk","precipBulkExpUncert","insuffDataQF",
                                "extremePrecipQF","heaterErrorQF","dielNoiseQF","strainGaugeStabilityQF",
                                "evapDetectedQF","inletHeater1QM","inletHeater2QM","inletHeater3QM",
                                "inletHeaterNAQM","finalQF")]
  
  
  
  
  # For the central day and adjacent days on either side of the central day, output the results. 
  # A later module will average output for each day
  dayOut <- InfoDirIn$time+as.difftime(c(-1,0,1),units='days')
  
  for(idxDayOut in seq_len(length(dayOut))){
    
    dayOutIdx <- dayOut[idxDayOut]
    
    # Get the records for this date
    setOutHour <- statsAgrHour$startDateTime >= dayOutIdx & 
      statsAgrHour$startDateTime < (dayOutIdx + as.difftime(1,units='days'))
    setOutDay <- statsAgrDay$date >= dayOutIdx & 
      statsAgrDay$date < (dayOutIdx + as.difftime(1,units='days'))
    
    
    # Filter the data for this output day
    statsAgrHourIdx <- statsAgrHour[setOutHour,]
    statsAgrDayIdx <- statsAgrDay[setOutDay,]
    
    # Final check whether uncertainty estimates are provided for all non-NA daily precip output 
    if(!(all.equal(is.na(statsAgrHourIdx$precipBulkExpUncert),is.na(statsAgrHourIdx$precipBulk)) == TRUE)){
      log$error(paste0('There is a mismatch between NA values in hourly precipBulk vs. precipBulkExpUncert for datum: ',
                       DirIn))
      stop()
    }
    # Final check whether uncertainty estimates are provided for all non-NA daily precip output
    if(!(all.equal(is.na(statsAgrDayIdx$precipBulkExpUncert),is.na(statsAgrDayIdx$precipBulk)) == TRUE)){
      log$error(paste0('There is a mismatch between NA values in daily precipBulk vs. precipBulkExpUncert for datum: ',
                       DirIn))
      stop()
    }
    
    
    # Replace the date in the output path structure with the current date
    dirOutStatIdx <- sub(pattern=format(InfoDirIn$time,'%Y/%m/%d'),
                         replacement=format(dayOutIdx,'%Y/%m/%d'),
                         x=dirOutStat,
                         fixed=TRUE)
    base::dir.create(dirOutStatIdx,recursive = TRUE)

    # Get the filename for this day
    nameFileIdx <- fileData[grepl(format(dayOutIdx,'%Y-%m-%d'),fileData)][1]

    if(!is.na(nameFileIdx)){
      
      # Append the center date to the end of the file name to know where it came from
      nameFileIdxSplt <- base::strsplit(nameFileIdx,'.',fixed=TRUE)[[1]]
      nameFileStatIdxSplt060 <- c(paste0(nameFileIdxSplt[1:(length(nameFileIdxSplt)-1)],
                                      '_stats_060',
                                  '_from_',
                                  format(InfoDirIn$time,'%Y-%m-%d')),
                           utils::tail(nameFileIdxSplt,1))
      nameFileStatIdxSplt01D <- c(paste0(nameFileIdxSplt[1:(length(nameFileIdxSplt)-1)],
                                         '_stats_01D',
                                         '_from_',
                                         format(InfoDirIn$time,'%Y-%m-%d')),
                                  utils::tail(nameFileIdxSplt,1))
      nameFileStatOut060Idx <- base::paste0(nameFileStatIdxSplt060,collapse='.')
      nameFileStatOut01DIdx <- base::paste0(nameFileStatIdxSplt01D,collapse='.')
      
      # Write out the data to file
      fileStatOut060Idx <- fs::path(dirOutStatIdx,nameFileStatOut060Idx)
      fileStatOut01DIdx <- fs::path(dirOutStatIdx,nameFileStatOut01DIdx)
      
      rptWrte <-
        base::try(NEONprocIS.base::def.wrte.parq(
          data = statsAgrHourIdx,
          NameFile = fileStatOut060Idx,
          NameFileSchm=NULL,
          Schm=SchmStatHour,
          log=log
        ),
        silent = TRUE)
      if ('try-error' %in% base::class(rptWrte)) {
        log$error(base::paste0(
          'Cannot write output to ',
          fileStatOut060Idx,
          '. ',
          attr(rptWrte, "condition")
        ))
        stop()
      } else {
        log$info(base::paste0(
          'Wrote hourly precipitation to file ',
          fileStatOut060Idx
        ))
      }

      rptWrte <-
        base::try(NEONprocIS.base::def.wrte.parq(
          data = statsAgrDayIdx,
          NameFile = fileStatOut01DIdx,
          NameFileSchm=NULL,
          Schm=SchmStatDay,
          log=log
        ),
        silent = TRUE)
      if ('try-error' %in% base::class(rptWrte)) {
        log$error(base::paste0(
          'Cannot write output to ',
          fileStatOut01DIdx,
          '. ',
          attr(rptWrte, "condition")
        ))
        stop()
      } else {
        log$info(base::paste0(
          'Wrote daily precipitation to file ',
          fileStatOut01DIdx
        ))
      }
      
      # Write out the flags to file. We only need the central day.
      if(idxDayOut == 2){
        setOutQf <- qfs$readout_time >= dayOutIdx & 
          qfs$readout_time < (dayOutIdx + as.difftime(1,units='days'))
        qfIdx <- qfs[setOutQf,]
        base::dir.create(dirOutQf,recursive = TRUE)
        nameFileQfIdxSplt <- c(paste0(nameFileIdxSplt[1:(length(nameFileIdxSplt)-1)],
                                         '_flagsSmooth'),
                                  utils::tail(nameFileIdxSplt,1))
        nameFileQfOutIdx <- base::paste0(nameFileQfIdxSplt,collapse='.')
        
        
        
        FileQfOutIdx <- fs::path(dirOutQf,nameFileQfOutIdx)
        
        rptWrte <-
          base::try(NEONprocIS.base::def.wrte.parq(
            data = qfIdx,
            NameFile = FileQfOutIdx,
            NameFileSchm=NULL,
            Schm=SchmQfGage,
            log=log
          ),
          silent = TRUE)
        if ('try-error' %in% base::class(rptWrte)) {
          log$error(base::paste0(
            'Cannot write output to ',
            FileQfOutIdx,
            '. ',
            attr(rptWrte, "condition")
          ))
          stop()
        } else {
          log$info(base::paste0(
            'Wrote flags to file ',
            FileQfOutIdx
          ))
        }
      }
        
    } else {
      log$warn(paste0(nameFileIdx,' and associated flags files are not able to be output because this data file was not found in the input.'))
    }        

  }

  return()
} 
