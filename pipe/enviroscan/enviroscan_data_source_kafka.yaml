---
pipeline:
  name: enviroscan_data_source_kafka
transform:
  image: us-central1-docker.pkg.dev/neon-shared-service/neonscience/neon-is-kfka-load-arry-pars:v1.2.3
  image_pull_secrets:
  - battelleecology-quay-read-all-pull-secret
  env:
    # environment variables for kafka loader
    OUT_PATH: /pfs/out
    SOURCE_TYPE: "enviroscan"
    LOG_LEVEL: INFO
    KAFKA_RETENTION_DAYS: "15"
    # environment variables for the array parser
    PARSE_CALIBRATION: 'False' 
    TEST_MODE: 'False'
    SOURCE_TYPE_INDEX: '3'
    SOURCE_TYPE_OUT: "enviroscan"
    YEAR_INDEX: '5'
    MONTH_INDEX: '6'
    DAY_INDEX: '7'
    SOURCE_ID_INDEX: '8'
    DATA_TYPE_INDEX: '9'
  secrets:
  - name: pachyderm-kafka-auth
    env_var: KAFKA_USER
    key: KAFKA_USER
  - name: pachyderm-kafka-auth
    env_var: KAFKA_PASSWORD
    key: KAFKA_PASSWORD
  - name: pachyderm-kafka-auth
    env_var: KAFKA_BROKER
    key: KAFKA_BROKER
  - name: pachyderm-kafka-auth
    env_var: KAFKA_LOG_TOPIC
    key: KAFKA_LOG_TOPIC
  - name: l0-bucket
    env_var: BUCKET_NAME
    key: LO_BUCKET
  - name: pdr-secret
    env_var: PDR_HOST
    key: hostname
  - name: pdr-secret
    env_var: PDR_DBNAME
    key: database
  - name: pdr-secret
    env_var: PDR_USER
    key: username
  - name: pdr-secret
    env_var: PDR_PASSWORD
    key: password
  cmd:
  - sh
  - "-c"
  - |-
    /bin/bash <<'EOF'
    
    # Use bash-scrict mode. See http://redsymbol.net/articles/unofficial-bash-strict-mode/
    set -euo pipefail
    IFS=$'\n\t'

    # Get GCP zone
    meta=$(curl -sH "Metadata-Flavor: Google" "http://metadata/computeMetadata/v1/instance/zone")
    zone=$(echo $meta | cut -d "/" -f 4)
    echo $zone
    
    # Get today's date for evaluating kafka data retention period
    date_today=$(date -u +%Y-%m-%d)
    kafka_min_date=$(date -u -d "$KAFKA_RETENTION_DAYS days ago" +%Y-%m-%d)
    
    # Get date from input path. Terminal path structure must be /SOURCE_TYPE/YYYY/MM/DD/SITE_FILE
    # Datum must be set at /SOURCE_TYPE/YYYY/MM/DD or /SOURCE_TYPE/YYYY/MM/DD/SITE_FILE
    date_path=$(echo $import_trigger | cut -f 5,6,7 -d "/")
    echo $date_path
    date_str=$(date -u +%Y-%m-%d -d $date_path)
    
    # Get each site to run
    if [[ -f ${import_trigger} ]]; then
      import_trigger_glob="${import_trigger}"
    else
      import_trigger_glob="${import_trigger}/*"
    fi
    
    mkdir -p "${OUT_PATH}/${SOURCE_TYPE}"

    cd app
    for site_kafka in $import_trigger_glob; do
      site_file=$(basename $site_kafka) # Strip off any path prefix
      site=$(echo $site_file | cut -f 1 -d "." --only-delimited) # Extract the site from site.kafka. Ignore site-only files (e.g. CPER vs. CPER.kafka)
      type=$(echo $site_file | cut -f 2 -d "." --only-delimited) # Extract the 'kafka' from site.kafka
      if [ "$type" != "kafka" ]
      then
        echo "$site_file is not indicated to be streaming from Kafka. Skipping..."
        continue
      elif [ "$(date -u +%s -d "$date_str")" -lt "$(date -u +%s -d "$kafka_min_date")" ]
      then
        echo -n "Cannot extract $date_str Kafka data for $site. "
        echo -n "Data date ($date_str) is beyond the Kafka retention period ($KAFKA_RETENTION_DAYS days before $date_today). Skipping..."
        continue
      fi
      
      # We are ok to run
      echo "Extracting $date_str kafka data for $site"

      # Make a directory to store the output files by site
      siteoutpath="${OUT_PATH}/${site}"
      mkdir -p "${siteoutpath}"
      
      # Get "current data" - data that came in on the specified day, which is the same day it was measured
      # Note: We cannot use the --removeoffset flag on the kafka loader (which removes the offsets from the filenames. This will often violate the Pachyderm requirement that different datums cannot write the same file)
      ./extract-kafka-sensor.py -s $site -S $SOURCE_TYPE -D "${siteoutpath}/${SOURCE_TYPE}_raw" -d $date_str --only current --consumer client.rack=$zone
      
      # Get "non-current data" - data that came in on the specified day, which is NOT the same day it was measured
      date_str_1=$(date +%Y-%m-%d -d "$date_str + 1 day")
      ./extract-kafka-sensor.py -s $site -S $SOURCE_TYPE -D "${siteoutpath}/${SOURCE_TYPE}_raw" -d $date_str_1 --only noncurrent --consumer client.rack=$zone
    
      # Upload L0 files to bucket, compacting with any existing file with the same name
      if [[ -d "${siteoutpath}/${SOURCE_TYPE}_raw" ]]; then
        linkdir=$(mktemp -d)
        shopt -s globstar
        out_parquet_glob="${siteoutpath}/**/*.parquet"
        # /pfs/out/ABBY/enviroscan/2023/01/01/12345/data/file.parquet
        echo "Linking output files to ${linkdir}"
        # set -x # Uncomment for debugging
        for f in $out_parquet_glob; do
          # Parse the path
          [[ "$f" =~ ^${siteoutpath}/(.*)/([0-9]+)/([0-9]+)/([0-9]+)/(.*)/data/(.*)$ ]]
          fsourcetype="${BASH_REMATCH[1]}"
          fyear="${BASH_REMATCH[2]}"
          fmonth="${BASH_REMATCH[3]}"
          fday="${BASH_REMATCH[4]}"
          fsourceid="${BASH_REMATCH[5]}"
          fname="${BASH_REMATCH[6]}"
          # fname_out="${SOURCE_TYPE}_${fsourceid}_${fyear}-${fmonth}-${fday}.parquet"  # Remove offsets from the filename
          outdir="${linkdir}/v2/${fsourcetype}/ms=${fyear}-${fmonth}/source_id=${fsourceid}"
          mkdir -p "${outdir}"
          ln -s "${f}" "${outdir}/${fname}"

        done

        # Upload to bucket, compacting with any existing file
        ./compact-bucket-copy.py --sourcepath "${linkdir}" --destbucket "${BUCKET_NAME}" --stripoffset

        # set +x # Uncomment for debugging
        rm -rf $linkdir
      fi
      
      # run data parser
      if [[ -d "${siteoutpath}/${SOURCE_TYPE}_raw" ]]; then
        # Run array parser 
        cd ../
        # export LOG_LEVEL=INFO
        export SCHEMA_PATH=$FILE_SCHEMA_PARSED
        export DATA_PATH=${siteoutpath}/${SOURCE_TYPE}_raw
        export OUT_PATH_ORIG=${OUT_PATH}
        export OUT_PATH=${siteoutpath} # set temporarily for use in the data parser
        echo "Running array parser"
        python3 -m array_parser.array_parser_main
        export OUT_PATH=${OUT_PATH_ORIG} # Return to what is set in the env vars

        # save parsed data to gcs 
        cd app
        
        linkdir=$(mktemp -d)
        shopt -s globstar
        out_parquet_glob="${siteoutpath}/${SOURCE_TYPE}/**/*.parquet"
        # /pfs/out/ABBY/enviroscan/2023/01/01/12345/data/file.parquet
        echo "Linking output files to ${linkdir}"
        # set -x
        for f in $out_parquet_glob; do
          # Parse the path
          [[ "$f" =~ ^${siteoutpath}/(.*)/([0-9]+)/([0-9]+)/([0-9]+)/(.*)/(.*)/(.*)$ ]]
          fsourcetype="${BASH_REMATCH[1]}"
          fyear="${BASH_REMATCH[2]}"
          fmonth="${BASH_REMATCH[3]}"
          fday="${BASH_REMATCH[4]}"
          fsourceid="${BASH_REMATCH[5]}"
          fname="${BASH_REMATCH[7]}"
          # fname_out="${fsourcetype}_${fsourceid}_${fyear}-${fmonth}-${fday}.parquet"  # Remove offsets from the filename
          outdir="${linkdir}/v2/${fsourcetype}/ms=${fyear}-${fmonth}/source_id=${fsourceid}"
          mkdir -p "${outdir}"
          ln -s "${f}" "${outdir}/${fname}"
          
        done
        
        # Upload to bucket, compacting with any existing file 
        ./compact-bucket-copy.py --sourcepath "${linkdir}" --destbucket "${BUCKET_NAME}" --stripoffset

        # Update the airflow triggering table
        echo "Updating airflow trigger table for ${site} ${date_str}"
        ./update-trigger-table.py -s "${site}" -S "${SOURCE_TYPE}" -D "${siteoutpath}/${SOURCE_TYPE}"
        
        # set +x # Uncomment for debugging
        
        rm -rf "${linkdir}"
        # Move the per site files to the structure expected by pachyderm
        cd "${siteoutpath}/${SOURCE_TYPE}_raw/"
        dest="${OUT_PATH}/${SOURCE_TYPE}_raw/"
        find . -type f | while read f; do
            d=$(dirname "$f")
            file=$(basename "$f")
            mkdir -p "$dest/$d"
            mv -f "$f" "$dest/$d/"
        done
        cd -
        cd "${siteoutpath}/${SOURCE_TYPE}/"
        dest="${OUT_PATH}/${SOURCE_TYPE}/"
        find . -type f | while read f; do
            d=$(dirname "$f")
            file=$(basename "$f")
            mkdir -p "$dest/$d"
            mv -f "$f" "$dest/$d/"
        done
        cd -
        # Remove the site directory
        rm -rf "${siteoutpath}"
      fi
    
    done
    EOF
input:
  cross:
  - pfs:
      name: FILE_SCHEMA_PARSED
      repo: enviroscan_avro_schemas
      glob: "/enviroscan/enviroscan_parsed.avsc"
  - pfs:
      name: import_trigger
      repo: enviroscan_cron_daily_and_date_control
      # Must be datum by day (e.g. /SOURCE_TYPE/*/*/*) or by day/site (e.g. /SOURCE_TYPE/*/*/*/*)
      glob: "/enviroscan/*/*/*"
parallelism_spec:
  constant: 3
autoscaling: true
resource_requests:
  memory: 500M
  cpu: 1.6
resource_limits:
  memory: 1.5G
  cpu: 2
sidecar_resource_requests:
  memory: 3G
  cpu: 0.5
datum_set_spec:
  number: 1
scheduling_spec:
  node_selector:
    cloud.google.com/compute-class: pach-pipeline-class
