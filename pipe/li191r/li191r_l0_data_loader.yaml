---
pipeline:
  name: li191r_l0_data_loader
transform:
  image_pull_secrets:
  - battelleecology-quay-read-all-pull-secret
  image: quay.io/battelleecology/data_source_trino:3f847d9d
  cmd:
  - sh
  - "-c"
  - |-
    /bin/bash <<'EOF'
    # Use bash-scrict mode. See http://redsymbol.net/articles/unofficial-bash-strict-mode/
    set -euo pipefail
    IFS=$'\n\t'
    # Refresh interim directories with each datum (otherwise they persist and cause probs)
    interimDir="/tmp/interimData"
    rm -rf $interimDir
    # Get today's date for evaluating kafka data retention period
    date_today=$(date -u +%Y-%m-%d)
    for path in $(find -L $import_trigger -type f); do
      echo "Processing $path"
      p=${path#/pfs}
      IFS="/"; arr=($p); unset IFS;
      year=${arr[3]}
      month=${arr[4]}
      day=${arr[5]}
      site=${arr[6]}
      echo "Extracting data from Trino for $year/$month/$day/$site"
      export download_year = $year
      export download_month = $month
      export download_day = $day
      export GEN_DATE=$year-$month-$day
      export GEN_SITE_NAME=$site
      export GEN_OUTPUT_DIR=$interimDir/$SOURCE_TYPE/$year/$month/$day
      export REQUESTS_CA_BUNDLE=/etc/pki/tls/cert.pem
      mkdir -p $GEN_OUTPUT_DIR
    done
      python3 -m l0_data_loader.l0_data_loader

  env:
    # Environment variables for data conversion step
    LOG_LEVEL: INFO
    REQUESTS_CA_BUNDLE: "/etc/pki/tls/cert.pem"
    # Environment variables for linkmerge step
    IN_PATH: /tmp/interimData
    OUT_PATH: /pfs/out
    # Environment variables for bash code
    SOURCE_TYPE: 'li191r'

  secrets:
  - name: l0-bucket
    env_var: BUCKET_NAME
    key: L0_BUCKET
input:
  cron:
    name: li191r_l0_data_loader_tick
    spec: "@every 5m"
output_branch: master
parallelism_spec:
  constant: 1
autoscaling: true
resource_requests:
  memory: 400M
  cpu: 1.2
resource_limits:
  memory: 800M
  cpu: 2
sidecar_resource_requests:
  memory: 3G
  cpu: 0.5
datum_set_spec:
  number: 1
scheduling_spec:
  node_selector:
    cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
    nodepool.neonscience.org/pipeline: "yes"
    cloud.google.com/gke-spot: "true"
pod_spec: |-
  { "tolerations": [
    {
      "key": "nodepool.neonscience.org/pipeline",
      "operator": "Exists"
    },
    {
      "effect": "NoSchedule",
      "key": "cloud.google.com/gke-spot",
      "operator": "Exists"
    }  
  ] }
