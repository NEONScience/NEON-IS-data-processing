---
pipeline:
  name: tchain_calibration_group_and_convert
transform:
  image: us-central1-docker.pkg.dev/neon-shared-service/neonscience/neon-is-cal-grp-conv:v2.3.1
  cmd:
  - sh
  - "-c"
  - |-
    /bin/bash <<'EOF'
    # Use bash-scrict mode. See http://redsymbol.net/articles/unofficial-bash-strict-mode/
    set -euo pipefail
    IFS=$'\n\t'
    
    # Refresh interim directories with each datum (otherwise they persist and cause probs)
    rm -rf $OUT_PATH_JOINER
    rm -rf /tmp/kafka_merged
    rm -rf /tmp/kafka_merged_copy
    rm -rf $OUT_PATH_ARRAY_PARSER
    mkdir -p $OUT_PATH_JOINER 
    mkdir -p /tmp/kafka_merged # Filter joiner relies on the same path positions among inputs (i.e. repo name in 2nd position)
    mkdir -p $OUT_PATH_ARRAY_PARSER

    # Join files from the archive and from Kafka
    export OUT_PATH=$OUT_PATH_JOINER
    python3 -m filter_joiner.filter_joiner_main

    # Combine data from Kafka and the archive
    Rscript ./flow.kfka.comb.R \
          DirIn=$OUT_PATH_JOINER \
          DirOut=/tmp/kafka_merged \
          DirErr=/pfs/out/errored_datums \
          FileSchmL0=$FILE_SCHEMA_PARSED \
          DirSubCopy=calibration

    # Copy and destroy links (cannot daisy chain links)
    cp -rL /tmp/kafka_merged /tmp/kafka_merged_copy || : # Allow to fail without exit code (happens if step above produced no output)

    # Run array parser (parse calibrations only - data already parsed)
    export SCHEMA_PATH=$FILE_SCHEMA_PARSED
    export DATA_PATH=/tmp/kafka_merged_copy
    export OUT_PATH=$OUT_PATH_ARRAY_PARSER
    python3 -m array_parser.array_parser_main
    

    # Run calibration conversion module
    Rscript ./flow.cal.conv.R \
      DirIn=$OUT_PATH_ARRAY_PARSER \
      DirOut=/pfs/out \
      DirErr=/pfs/out/errored_datums \
      "FileSchmData=$FILE_SCHEMA_PARSED" \
      "FileSchmQf=$FILE_SCHEMA_FLAGS" \
      "TermFuncConv=depth0WaterTemp:def.cal.conv.poly|depth1WaterTemp:def.cal.conv.poly|depth2WaterTemp:def.cal.conv.poly|depth3WaterTemp:def.cal.conv.poly|depth4WaterTemp:def.cal.conv.poly|depth5WaterTemp:def.cal.conv.poly|depth6WaterTemp:def.cal.conv.poly|depth7WaterTemp:def.cal.conv.poly|depth8WaterTemp:def.cal.conv.poly|depth9WaterTemp:def.cal.conv.poly|depth10WaterTemp:def.cal.conv.poly" \
      "TermQf=depth0WaterTemp|depth1WaterTemp|depth2WaterTemp|depth3WaterTemp|depth4WaterTemp|depth5WaterTemp|depth6WaterTemp|depth7WaterTemp|depth8WaterTemp|depth9WaterTemp|depth10WaterTemp" \
      "TermFuncUcrt=depth0WaterTemp:def.ucrt.meas.cnst|depth1WaterTemp:def.ucrt.meas.cnst|depth2WaterTemp:def.ucrt.meas.cnst|depth3WaterTemp:def.ucrt.meas.cnst|depth4WaterTemp:def.ucrt.meas.cnst|depth5WaterTemp:def.ucrt.meas.cnst|depth6WaterTemp:def.ucrt.meas.cnst|depth7WaterTemp:def.ucrt.meas.cnst|depth8WaterTemp:def.ucrt.meas.cnst|depth9WaterTemp:def.ucrt.meas.cnst|depth10WaterTemp:def.ucrt.meas.cnst"

    EOF
  env:
    # Environment variables for filter-joiner
    CONFIG: |
      ---
      # Configuration for filter-joiner module that will bring together the data and calibrations
      # In Pachyderm root will be index 0, 'pfs' index 1, and the repo name index 2.
      # Metadata indices will typically begin at index 3.
      input_paths:
        - path:
            name: KAFKA_UNMERGED_DATA
            # Filter for data directory
            glob_pattern: /pfs/KAFKA_UNMERGED_DATA/tchain/*/*/*/*/**
            # Join on named location (already joined below by day)
            join_indices: [7]
            outer_join: true
        - path:
            name: DATA_PATH_ARCHIVE
            # Filter for data directory
            glob_pattern: /pfs/DATA_PATH_ARCHIVE/tchain/*/*/*/*/**
            # Join on named location (already joined below by day)
            join_indices: [7]
            outer_join: true
        - path:
            name: CALIBRATION_PATH
            # Filter for data directory
            glob_pattern: /pfs/CALIBRATION_PATH/tchain/*/*/*/*/**
            # Join on named location (already joined below by day)
            join_indices: [7]
            outer_join: true
    LOG_LEVEL: INFO
    RELATIVE_PATH_INDEX: "3" # Must be consistent across inputs 
    LINK_TYPE: COPY # options are COPY or SYMLINK. MUST BE SIMLINK IF USING COMBINED MODULE.
    OUT_PATH_JOINER: /tmp/pfs/filter_joined
    
    # Environment variables for array parser
    PARSE_CALIBRATION: 'True'
    TEST_MODE: 'True' # We already parsed the data. Pass it through via symlink
    LOG_LEVEL: DEBUG
    SOURCE_TYPE_INDEX: '3'
    YEAR_INDEX: '4'
    MONTH_INDEX: '5'
    DAY_INDEX: '6'
    SOURCE_ID_INDEX: '7'
    DATA_TYPE_INDEX: '8'
    OUT_PATH_ARRAY_PARSER: /tmp/pfs/array_parsed # R modules must have pfs in the repo structure

    # Environment variables for calibration module
    PARALLELIZATION_INTERNAL: '1' # Option for calibration conversion module
input:
  cross:
  - pfs:
      name: FILE_SCHEMA_PARSED
      repo: tchain_avro_schemas
      glob: /tchain/tchain_parsed.avsc
  - pfs:
      name: FILE_SCHEMA_FLAGS
      repo: tchain_avro_schemas
      glob: /tchain/tchain_flags_calibration.avsc
  # Outer join all repos so that varying sensors between kafka and trino loaders will all get joined with calibrations. Filter-joiner will narrow down.
  - join:
    - pfs:
        name: CALIBRATION_PATH
        repo: tchain_calibration_assignment
        glob: /tchain/(*)/(*)/(*)
        joinOn: $1/$2/$3
        outer_join: true
        empty_files: false # Make sure to use false if LINK_TYPE=COPY. Can also be set to false for LINK_TYPE=SYMLINK.
    - pfs:
        name: DATA_PATH_ARCHIVE
        repo: tchain_data_source_gcs
        glob: /tchain/(*)/(*)/(*)
        joinOn: $1/$2/$3
        outer_join: true
        empty_files: false # Make sure to use false if LINK_TYPE=COPY. Can also be set to false for LINK_TYPE=SYMLINK.
    - pfs:
        name: KAFKA_UNMERGED_DATA
        repo: tchain_data_source_kafka
        glob: /tchain/(*)/(*)/(*)
        joinOn: $1/$2/$3
        outer_join: true
        empty_files: false # Make sure to use false if LINK_TYPE=COPY. Can also be set to false for LINK_TYPE=SYMLINK.
parallelism_spec:
  constant: 5
autoscaling: true
resource_requests:
  memory: 1.2G
  cpu: 1.3
resource_limits:
  memory: 2G
  cpu: 2
sidecar_resource_requests:
  memory: 1G
  cpu: 0.5
datum_set_spec:
  number: 1
scheduling_spec:
  node_selector:
    cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
    nodepool.neonscience.org/pipeline: "yes"
    cloud.google.com/gke-spot: "true"
pod_spec: |-
  { "tolerations": [
    {
      "key": "nodepool.neonscience.org/pipeline",
      "operator": "Exists"
    },
    {
      "effect": "NoSchedule",
      "key": "cloud.google.com/gke-spot",
      "operator": "Exists"
    }  
  ] }
