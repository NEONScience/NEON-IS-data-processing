---
pipeline:
  name: mti300ahrs_structure_and_merge_by_location
transform:
  image: us-central1-docker.pkg.dev/neon-shared-service/neonscience/neon-is-loc-grp-strc-comb:v1.2.2
  cmd:
  - sh
  - "-c"
  - |-
    /bin/bash <<'EOF'
    # Use bash-scrict mode. See http://redsymbol.net/articles/unofficial-bash-strict-mode/
    set -euo pipefail
    IFS=$'\n\t'
    # Refresh interim directories with each datum (otherwise they persist and cause probs)
    rm -rf /tmp/pfs/structured
    rm -rf /tmp/pfs/structuredCopy
    mkdir -p /tmp/pfs/structured
    # Run second module - structure repo by location
    Rscript ./flow.loc.repo.strc.R \
      DirIn=$DIR_IN \
      DirOut=/tmp/pfs/structured \
      DirErr=/pfs/out/errored_datums \
      Comb=TRUE
    # Copy output to another interim folder to destroy links (cannot daisy chain links from pfs input to output)
    cp -rL /tmp/pfs/structured /tmp/pfs/structuredCopy  || : # Allow to fail without exit code (happens if step above produced no output)
    rm -rf /tmp/pfs/structured 
    # Run third module - merge data by location
    Rscript ./flow.loc.data.trnc.comb.R \
      DirIn=/tmp/pfs/structuredCopy \
      DirOut=/pfs/out \
      DirErr=/pfs/out/errored_datums \
      "DirSubCombData=data|flags" \
      DirSubCopy=location
    EOF
  env:
    # Environment variables for R modules
    PARALLELIZATION_INTERNAL: '1' 
input:
  pfs:
    name: DIR_IN
    repo: mti300ahrs_data_location_group
    glob: /mti300ahrs/*/*/*
parallelism_spec:
  constant: 2
autoscaling: true
resource_requests:
  memory: 5G
  cpu: 1.5
resource_limits:
  memory: 7G
  cpu: 2.5
sidecar_resource_requests:
  memory: 3G
  cpu: 1.5
datum_set_spec:
  number: 1
scheduling_spec:
  node_selector:
    cloud.google.com/compute-class: pach-pipeline-class
