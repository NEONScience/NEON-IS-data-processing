---
pipeline:
  name: dualfan_location_group_and_restructure
transform:
  image: us-central1-docker.pkg.dev/neon-shared-service/neonscience/neon-is-loc-grp-strc-comb:sha-677b4b3
  cmd:
  - sh
  - "-c"
  - |-
    /bin/bash <<'EOF'
    # Use bash-scrict mode. See http://redsymbol.net/articles/unofficial-bash-strict-mode/
    set -euo pipefail
    IFS=$'\n\t'
    # Refresh interim directories with each datum (otherwise they persist and cause probs)
    rm -rf /tmp/pfs/filter_joined
    rm -rf /tmp/pfs/structured
    rm -rf /tmp/pfs/structuredCopy
    rm -rf /tmp/kafka_merged
    rm -rf $OUT_PATH
    mkdir -p /tmp/kafka_merged # Filter joiner relies on the same path positions among inputs (i.e. repo name in 2nd position)
    mkdir -p $OUT_PATH # R modules must have pfs in the repo structure
    mkdir -p /tmp/pfs/filter_joined
    if [ ${DATA_PATH_TRINO+x} ]; then 
      # Data from trino. 
      # Set CONFIG for filter-joiner to the trino version and run filter joiner
      export CONFIG=$CONFIG_TRINO
      python3 -m filter_joiner.filter_joiner_main
    fi  
    if [ ${KAFKA_UNMERGED_DATA+x} ]; then 
      # Data from kafka. 
      # Run kafka combiner
      Rscript ./flow.kfka.comb.R \
          DirIn=$KAFKA_UNMERGED_DATA \
          DirOut=/tmp/kafka_merged \
          DirErr=/pfs/out/errored_datums \
          FileSchmL0=$FILE_SCHEMA_L0
      # Set CONFIG for filter-joiner to the kafka version and run filter joiner
      export CONFIG=$CONFIG_KAFKA
      python3 -m filter_joiner.filter_joiner_main
    fi  
    # Run second module - structure repo by location
    Rscript ./flow.loc.repo.strc.R \
      DirIn=/tmp/pfs/filter_joined \
      DirOut=/tmp/pfs/structured \
      DirErr=/pfs/out/errored_datums \
      Comb=TRUE
    # Copy output to another interim folder to destroy links (cannot daisy chain links from pfs input to output)
    cp -rL /tmp/pfs/structured /tmp/pfs/structuredCopy || : # Allow to fail without exit code (happens if step above produced no output)
    rm -rf /tmp/pfs/filter_joined 
    rm -rf /tmp/pfs/structured 
    # Run third module - merge data by location
    Rscript ./flow.loc.data.trnc.comb.R \
      DirIn=/tmp/pfs/structuredCopy \
      DirOut=/pfs/out \
      DirErr=/pfs/out/errored_datums \
      "DirSubCombData=data|flags|uncertainty_data" \
      DirSubCombUcrt=uncertainty_coef \
      DirSubCopy=location 
    EOF
  env:
    # Environment variables for filter-joiner
    CONFIG_TRINO: |
      ---
      # Configuration for filter-joiner module that will bring together the data and calibrations
      # In Pachyderm root will be index 0, 'pfs' index 1, and the repo name index 2.
      # Metadata indices will typically begin at index 3.
      input_paths:
        - path:
            name: DATA_PATH_TRINO
            # Filter for data directory
            glob_pattern: /pfs/DATA_PATH_TRINO/dualfan/*/*/*/*/**
            # Join on named location (already joined below by day)
            join_indices: [7]
            outer_join: true
        - path:
            name: LOCATION_PATH
            # Filter for data directory
            glob_pattern: /pfs/LOCATION_PATH/*/*/*/*/**
            # Join on named location (already joined below by day)
            join_indices: [7]
    CONFIG_KAFKA: |
      ---
      # Configuration for filter-joiner module that will bring together the data and calibrations
      # In Pachyderm root will be index 0, 'pfs' index 1, and the repo name index 2.
      # Metadata indices will typically begin at index 3.
      input_paths:
        - path:
            name: DATA_PATH_KAFKA
            # Filter for data directory
            glob_pattern: /tmp/kafka_merged/dualfan/*/*/*/*/**
            # Join on named location (already joined below by day)
            join_indices: [7]
            outer_join: true
        - path:
            name: LOCATION_PATH
            # Filter for data directory
            glob_pattern: /pfs/LOCATION_PATH/dualfan/*/*/*/*/**
            # Join on named location (already joined below by day)
            join_indices: [7]
    OUT_PATH: /tmp/pfs/filter_joined
    LOG_LEVEL: INFO
    RELATIVE_PATH_INDEX: "3"
    LINK_TYPE: COPY # options are COPY or SYMLINK. Use COPY for combined module.
    # Environment variables for R modules
    PARALLELIZATION_INTERNAL: '3' 
input:
  join:
  - pfs:
      name: LOCATION_PATH
      repo: dualfan_location_asset_assignment
      glob: /dualfan/(*)/(*)/(*)
      joinOn: $1/$2/$3
      outer_join: true
      empty_files: false # Make sure to use false if LINK_TYPE=COPY. Can also be set to false for LINK_TYPE=SYMLINK.
  - pfs:
      name: DATA_PATH_TRINO
      repo: dualfan_data_source_trino
      glob: /dualfan/(*)/(*)/(*)
      joinOn: $1/$2/$3
      outer_join: true
      empty_files: false # Make sure to use false if LINK_TYPE=COPY. Can also be set to false for LINK_TYPE=SYMLINK.
  - pfs:
      name: KAFKA_UNMERGED_DATA
      repo: dualfan_data_source_kafka
      glob: /dualfan/(*)/(*)/(*)
      joinOn: $1/$2/$3
      outer_join: true
      empty_files: false
      
parallelism_spec:
  constant: 5
autoscaling: true
resource_requests:
  memory: 2.2G
  cpu: 3.3
resource_limits:
  memory: 4G
  cpu: 4.5
sidecar_resource_requests:
  memory: 3G
  cpu: 0.5
datum_set_spec:
  number: 1
scheduling_spec:
  node_selector:
    cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
    nodepool.neonscience.org/pipeline: "yes"
    cloud.google.com/gke-spot: "true"
pod_spec: |-
  { "tolerations": [
    {
      "key": "nodepool.neonscience.org/pipeline",
      "operator": "Exists"
    },
    {
      "effect": "NoSchedule",
      "key": "cloud.google.com/gke-spot",
      "operator": "Exists"
    }  
  ] }
