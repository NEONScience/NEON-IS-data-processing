---
pipeline:
  name: hmp155_data_source_trino
transform:
  image_pull_secrets:
  - battelleecology-quay-read-all-pull-secret
  image: quay.io/battelleecology/data_source_trino:v0.0.3
  cmd:
  - /bin/bash
  stdin:
  - "#!/usr/bin/env bash"
  - '# Refresh interim directories with each datum (otherwise they persist and cause probs)'
  - rm -r -f /usr/src/app/interimData
  - '# Run first module - data_source_site (pull data from database by site)'
  - "# Split data source path"
  - echo "Checking $presto_data_source files"
  - for path in $(find -L $import_trigger -type f); do
  - '  echo "Processing $path"'
  - "  p=${path#/pfs}"
  - '  IFS="/"; arr=($p); unset IFS;'
  - "  year=${arr[2]}"
  - "  month=${arr[3]}"
  - "  day=${arr[4]}"
  - "  site=${arr[5]}"
  - "  export GEN_DATE=$year-$month-$day"
  - "  export GEN_SITE_NAME=$site"
  - "  export GEN_OUTPUT_DIR=/usr/src/app/interimData/hmp155/$year/$month/$day"
  - "  export REQUESTS_CA_BUNDLE=/etc/pki/tls/cert.pem"
  - "  mkdir -p $GEN_OUTPUT_DIR"
  - "  /usr/src/app/genscript/genparquet.py --storesitename --codec gzip --truncperiod second"
  - done
  - '# Run second module - parquet_linkmerge (merges data from a source id that collected data from multiple sites in one day'
  - python3 -m parquet_linkmerge.parquet_linkmerge_main
  env:
    # Environment variables for data conversion step
    GEN_YAML_CONF: "/usr/src/app/genscript/configs/hmp155_streams.yaml"
    GEN_SCHEMA_FILE: "/usr/src/app/schemas/hmp155/hmp155.avsc"
    LOG_LEVEL: info
    REQUESTS_CA_BUNDLE: "/etc/pki/tls/cert.pem"
    # Environment variables for linkmerge step
    IN_PATH: /usr/src/app/interimData
    OUT_PATH: /pfs/out
    SOURCE_TYPE_INDEX: '5'
    YEAR_INDEX: '6'
    MONTH_INDEX: '7'
    DAY_INDEX: '8'
    SOURCE_ID_INDEX: '9'
  secrets:
  - name: pachd-trino-secret
    key: TRINO_HOST
    env_var: PRESTO_HOST
  - name: pachd-trino-secret
    key: TRINO_PASSWORD
    env_var: PRESTO_PASSWORD
  - name: pachd-trino-secret
    key: TRINO_USER
    env_var: PRESTO_USER
input:
  pfs:
    name: import_trigger
    repo: import_trigger_hmp155
    glob: "/*/*/*"
output_branch: master
parallelism_spec:
  constant: 2
resource_requests:
  memory: 500M
  cpu: 0.7
autoscaling: true
scheduling_spec:
  node_selector:
    cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
    nodepool.neonscience.org/pipeline: "yes"
    cloud.google.com/gke-spot: "true"
pod_spec: |-
  { "tolerations": [
    {
      "key": "nodepool.neonscience.org/pipeline",
      "operator": "Exists"
    },
    {
      "effect": "NoSchedule",
      "key": "cloud.google.com/gke-spot",
      "operator": "Exists"
    }  
  ] }
pod_patch: |-
  [
    { "op": "replace",
      "path":"/containers/1/resources/requests/memory",
      "value":"1G"
    },
    { "op": "replace",
      "path": "/containers/1/resources/requests/cpu",
      "value": "0.3"
    }
  ]
