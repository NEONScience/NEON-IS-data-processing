---
pipeline:
  name: tempSpecificDepthLakes_level1_grp_cons_srf
transform:
  image: us-central1-docker.pkg.dev/neon-shared-service/neonscience/neon-is-levl1-grp-cons-srf:v2.2.1
  cmd:
  - sh
  - "-c"
  - |-
    /bin/bash <<'EOF'
    # Use bash-scrict mode. See http://redsymbol.net/articles/unofficial-bash-strict-mode/
    set -euo pipefail
    IFS=$'\n\t'
    
    # Refresh interim directories with each datum (otherwise they persist and cause probs)
    rm -r -f /tmp/pfs/filter_joined
    mkdir -p /tmp/pfs/filter_joined
    
    # Set some environment variables for the first module
    export OUT_PATH=$OUT_PATH
    
    # Run first module - filter-joiner (using environment variables below as input parameters)
    python3 -m filter_joiner.filter_joiner_main
    
    # Run second module - - pub workbook loader (using environment variables below as input parameters)
    python3 -m pub_workbook_loader.pub_workbook_loader_main
    
    # Run third module - create pub tables and apply science review flags (if any)
    Rscript ./flow.pub.tabl.srf.R \
      DirIn=/tmp/pfs/filter_joined \
      DirOut=/pfs/out \
      DirErr=/pfs/out/errored_datums \
      "DirData=data" \
      PathPubWb=$PUB_WORKBOOKS \
      "DirSubCopy=science_review_flags|group|location"
      
    # Export Level 1 data to bucket
    export OUT_PATH=/pfs/out
    linkdir=$(mktemp -d)
    shopt -s globstar
    out_parquet_glob="${OUT_PATH}/**/*.parquet"
    # Example: /2024/01/18/par-quantum-line_UKFS001000/data/par-quantum-line_UKFS001000_2024-01-18_PARQL_1min_001.parquet
    # Example: /2025/05/10/temp-specific-depths-lakes-split_BARC103501/data/temp-specific-depths-lakes-split_BARC103501_2025_05_10_TSD_1_min_001.parquet
    echo "Linking output files to ${linkdir}"
    set -x # Echo commands to output for debugging
    fname=""
    for f in $out_parquet_glob; do
      if [[ -f "$f" ]]; then
        # Parse the path
        [[ "$f" =~ ^$OUT_PATH/([0-9]+)/([0-9]+)/([0-9]+)/(${GROUP_PREFIX}_[A-Za-z0-9]+)/data/(.*)$ ]]
        fyear="${BASH_REMATCH[1]}"
        fmonth="${BASH_REMATCH[2]}"
        fday="${BASH_REMATCH[3]}"
        fgroup="${BASH_REMATCH[4]}"
        fname="${BASH_REMATCH[5]}"
        # Now get the timing index from the file name
        [[ "$fname" =~ ^${GROUP_PREFIX}_[A-Za-z0-9]+_${fyear}-${fmonth}-${fday}_[A-Za-z0-9]+_([A-Za-z0-9]+)+_([A-Za-z0-9]+)_([A-Za-z0-9]+).parquet ]]
        avg_int="${BASH_REMATCH[2]}"
        #Form the output path and link
        outdir="${linkdir}/v2/${GROUP_PREFIX}/${avg_int}/group=${fgroup}/ms=${fyear}-${fmonth}"
        mkdir -p "${outdir}"
        ln -s "${f}" "${outdir}/${fname}"
      fi
    done
    set +x
    if [[ "${fname}" ]]; then
      echo "Syncing files to bucket"
      rclone \
        --no-check-dest \
        --copy-links \
        --gcs-bucket-policy-only \
        --gcs-no-check-bucket \
        copy \
        "${linkdir}" \
        ":gcs://${BUCKET_NAME}"
      echo "Removing temporary files"
      rm -rf $linkdir
    fi
    
    EOF
  env:
    # Environment variables for filter-joiner
    CONFIG: |
      ---
      # In Pachyderm root will be index 0, 'pfs' index 1, and the repo name index 2.
      # Metadata indices will typically begin at index 3.
      # Use unix-style glob pattern to select the desired directories in each repo 
      
      input_paths:
        - path:
            name: GROUP_PATH
            # Filter for data directory
            glob_pattern: /pfs/GROUP_PATH/*/**
            # Join on Y/M/D/group ID 
            join_indices: [6]
            outer_join: True
        - path:
            name: SRF_PATH
            # Filter for data directory
            glob_pattern: /pfs/SRF_PATH/*/**
            # Join on group ID(already joined below by day)
            join_indices: [6]

    OUT_PATH: /tmp/pfs/filter_joined
    LOG_LEVEL: DEBUG
    RELATIVE_PATH_INDEX: "3"
    LINK_TYPE: COPY # options are COPY or SYMLINK. Use COPY for combined module.
    # Environment variables for calibration module
    PARALLELIZATION_INTERNAL: '1' # Option for stats module
    
    # Environment variables for pub_workbook_loader
    OUT_PATH_WORKBOOK: /tmp/pub_workbooks
    PRODUCTS: NEON.DOM.SITE.DP1.20264.001 # Format: NEON.DOM.SITE.DPX.XXXXX.XXX,NEON.DOM.SITE.DPX.XXXXX.XXX,etc
  
    # Environment variables for pub table and srf module
    PUB_WORKBOOKS: /tmp/pub_workbooks
    
    # Environment variables for the L1 archiver
    GROUP_PREFIX: temp-specific-depths-lakes-split # no ending "_"
    
    
  secrets:
  - name: pdr-secret
    mount_path: /var/db_secret
  - name: l1-bucket
    env_var: BUCKET_NAME
    key: L1_BUCKET
    
input:
  join:
    - pfs:
        name: GROUP_PATH
        repo: tempSpecificDepthLakes_level1_reshape
        glob: /(*)/(*)/(*)
        joinOn: $1/$2/$3
        outer_join: true # Need outer join to pull in with or without SRFs
        empty_files: false # Make sure this is false for LINK_TYPE=COPY
    - pfs:
        name: SRF_PATH
        repo: tempSpecificDepthLakes_srf_assignment
        glob: /(*)/(*)/(*)
        joinOn: $1/$2/$3
        empty_files: false # Make sure this is false for LINK_TYPE=COPY
parallelism_spec:
  constant: 5
resource_requests: 
  memory: 500M
  cpu: 1.2
resource_limits:
  memory: 1G
  cpu: 2
sidecar_resource_requests:
  memory: 1G
  cpu: 0.5
autoscaling: true
datum_set_spec:
  number: 1
scheduling_spec:
  node_selector:
    cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
    nodepool.neonscience.org/pipeline: "yes"
    cloud.google.com/gke-spot: "true"
pod_spec: |-
  { "tolerations": [
    {
      "key": "nodepool.neonscience.org/pipeline",
      "operator": "Exists"
    },
    {
      "effect": "NoSchedule",
      "key": "cloud.google.com/gke-spot",
      "operator": "Exists"
    }  
  ] }

