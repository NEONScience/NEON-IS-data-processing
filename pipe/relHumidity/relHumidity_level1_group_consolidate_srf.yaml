---
pipeline:
  name: relHumidity_level1_group_consolidate_srf
transform:
  image_pull_secrets:
  - battelleecology-quay-read-all-pull-secret
  image: quay.io/battelleecology/neon-is-levl1-grp-cons-srf:13a29e7cccbad53dacd59820953b930b27c6ffd7
  cmd:
  - "/bin/bash"
  stdin:
  - "#!/bin/bash"
  - '# Refresh interim directories with each datum (otherwise they persist and cause probs)'
  - rm -rf /tmp/interimA
  - rm -rf /tmp/interimB
  - rm -rf /tmp/pfs/interimC
  - mkdir /tmp/interimA
  - mkdir /tmp/interimB
  - mkdir -p /tmp/pfs/interimC
  - '# Set some environment variables for the first module'
  - export OUT_PATH=$OUT_PATH_1
  - '# Run first module - filter-joiner (using environment variables below as input parameters)'
  - python3 -m filter_joiner.filter_joiner_main
  - '# Set some environment variables for the second module'
  - export OUT_PATH=$OUT_PATH_2
  - export CONFIG=$CONFIG2
  - '# Run second module - filter-joiner to bring in SRF (using environment variables below as input parameters)'
  - python3 -m filter_joiner.filter_joiner_main
  - '# Clean up 1st interim directory (this is the only one we can clean up bc the rest use symlinks)'
  - rm -rf /tmp/interimA  
  - '# Set some environment variables for the 3rd module'
  - export OUT_PATH=$OUT_PATH_3
  - '# Run third module - level 1 consolidate (using environment variables below as input parameters)'
  - python3 -m level1_consolidate.level1_consolidate_main
  - '# Run fourth module - pub workbook loader (using environment variables below as input parameters)'
  - python3 -m pub_workbook_loader.pub_workbook_loader_main
  - '# Run fifth and final module - create pub tables and apply science review flags (if any)'
  - Rscript ./flow.pub.tabl.srf.R
      DirIn=/tmp/pfs/interimC
      DirOut=/pfs/out
      DirErr=/pfs/out/errored_datums
      "DirData=stats|quality_metrics"
      PathPubWb=$PUB_WORKBOOKS
      "DirSubCopy=science_review_flags|group|location"
  env:
    # Environment variables for 1st filter-joiner. Need to join by day again here because an outer join was used on 
    # these repos in order to pull them in with or without the SRF
    CONFIG: |
      ---
      # In Pachyderm root will be index 0, 'pfs' index 1, and the repo name index 2.
      # Metadata indices will typically begin at index 3.
      # Use unix-style glob pattern to select the desired directories in each repo 
      input_paths:
        - path:
            name: QUALITY_METRICS_PATH
            # Filter for data directory
            glob_pattern: /pfs/QUALITY_METRICS_PATH/*/*/*/*/hmp155/*/quality_metrics/**
            # Join on Y/M/D/group ID 
            join_indices: [3,4,5,6]
        - path:
            name: STATISTICS_PATH
            # Filter for data directory
            glob_pattern: /pfs/STATISTICS_PATH/*/*/*/*/hmp155/*/stats/**
            # Join on Y/M/D/group ID 
            join_indices: [3,4,5,6]
        - path:
            name: GROUP_PATH
            # Grab group information
            glob_pattern: /pfs/GROUP_PATH/*/*/*/*/group/**
            # Join on Y/M/D/group ID 
            join_indices: [3,4,5,6]
        - path:
            name: GROUP_PATH
            # Grab location information
            glob_pattern: /pfs/GROUP_PATH/*/*/*/*/*/*/location/**
            # Join on Y/M/D/group ID 
            join_indices: [3,4,5,6]
    OUT_PATH_1: /tmp/interimA # Transfered to OUT_PATH for the first module
    RELATIVE_PATH_INDEX: "3" # This is shared among the 2 filter joiners and consolidation module
    LINK_TYPE: COPY # options are COPY or SYMLINK. Use COPY for combined modules. Also shared with 2nd & 3rd modules 
    LOG_LEVEL: INFO # Shared among all modules

# Below are the environment variables for 2nd filter-joiner bringing in the Science review flags 
# Can't do this in first filter-joiner bc there are only data in the srf assignment
# repo for groups that have applicable SRFs for the day. Need to pass through the
# consolidated output with an outer join.
    CONFIG2: |
      ---
      # In Pachyderm root will be index 0, 'pfs' index 1, and the repo name index 2.
      # Metadata indices will typically begin at index 3.
      # Use unix-style glob pattern to select the desired directories in each repo 
      input_paths:
        - path:
            name: CONSOLIDATED_PATH
            # Filter for data directory
            glob_pattern: /tmp/interimA/*/*/*/*/**
            # Join on group ID (already joined below by day)
            join_indices: [6]
            outer_join: True
        - path:
            name: SRF_PATH
            # Filter for data directory
            glob_pattern: /pfs/SRF_PATH/*/*/*/*/**
            # Join on group ID(already joined below by day)
            join_indices: [6]
    OUT_PATH_2: /tmp/interimB # This will be transfered to OUT_PATH for the this module

# Environment variables for level 1 consolidation
    IN_PATH: /tmp/interimB
    OUT_PATH_3: /tmp/pfs/interimC # This will be transfered to OUT_PATH for the second module
    GROUP_INDEX: "6"
    # path index of names of group-level metadata to include in the output
    GROUP_METADATA_INDEX: "7"
    GROUP_METADATA_NAMES: group,science_review_flags
    # path index of names of directories to include in the output
    DATA_TYPE_INDEX: "9"
    DATA_TYPE_NAMES: location,stats,quality_metrics

# Environment variables for pub_workbook_loader
    OUT_PATH_WORKBOOK: /tmp/pub_workbooks
    PRODUCTS: NEON.DOM.SITE.DP1.00098.001,NEON.DOM.SITE.DP1.20271.001,NEON.DOM.SITE.DP1.20046.001 # Format: NEON.DOM.SITE.DPX.XXXXX.XXX,NEON.DOM.SITE.DPX.XXXXX.XXX,etc
  
# Environment variables for pub table and srf module
    PUB_WORKBOOKS: /tmp/pub_workbooks
    PARALLELIZATION_INTERNAL: '3'

  secrets:
  - name: pdr-secret
    mount_path: /var/db_secret
    
input:
  join:
  - pfs:
      name: QUALITY_METRICS_PATH
      repo: relHumidity_qm_group_and_compute
      glob: /(*)/(*)/(*)
      joinOn: $1/$2/$3
      outer_join: true # Need outer join to pull in with or without SRFs
      empty_files: false # Make sure this is false for LINK_TYPE=COPY
  - pfs:
      name: STATISTICS_PATH
      repo: relHumidity_stats_group_and_compute
      glob: /(*)/(*)/(*)
      joinOn: $1/$2/$3
      outer_join: true # Need outer join to pull in with or without SRFs
      empty_files: false # Make sure this is false for LINK_TYPE=COPY
  - pfs:
      name: GROUP_PATH
      repo: relHumidity_group_path
      glob: /(*)/(*)/(*)
      joinOn: $1/$2/$3
      outer_join: true # Need outer join to pull in with or without SRFs
      empty_files: false # Make sure this is false for LINK_TYPE=COPY
  - pfs:
      name: SRF_PATH
      repo: relHumidity_srf_assignment
      glob: /(*)/(*)/(*)
      joinOn: $1/$2/$3
      empty_files: false # Make sure this is false for LINK_TYPE=COPY
output_branch: master
parallelism_spec:
  constant: 5
autoscaling: true
resource_requests:
  memory: 1.2G
  cpu: 3.3
resource_limits:
  memory: 3G
  cpu: 4.5
sidecar_resource_requests:
  memory: 1G
  cpu: 0.5
sidecar_resource_limits:
  memory: 2Gi
  cpu: 1.3
datum_set_spec:
  number: 1
scheduling_spec:
  node_selector:
    cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
    nodepool.neonscience.org/pipeline: "yes"
    cloud.google.com/gke-spot: "true"
pod_spec: |-
  { "tolerations": [
    {
      "key": "nodepool.neonscience.org/pipeline",
      "operator": "Exists"
    },
    {
      "effect": "NoSchedule",
      "key": "cloud.google.com/gke-spot",
      "operator": "Exists"
    }  
  ] }
