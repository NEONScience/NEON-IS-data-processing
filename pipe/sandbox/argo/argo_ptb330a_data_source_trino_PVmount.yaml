# Note: Currently requires running with argo service account at the worflow level due to the default SA not having permissions to create persistent volumes
# argo submit --serviceaccount argo -n argo --watch argo_ptb330a_data_source_trino_volumeMount.yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ptb330a-data-source-trino-pvmount-
spec:
  entrypoint: trino-load-example
  templates:
  - name: trino-load-example
    steps:
    - - name: generate-volume
        template: generate-volume
        arguments:
          parameters:
            - name: pvc-size
              # In a real-world example, this could be generated by a previous workflow step.
              value: '200M'
    - - name: load-data
        template: load-from-trino
        arguments:
          parameters:
            - name: pvc-name
              value: '{{steps.generate-volume.outputs.parameters.pvc-name}}'
    - - name: merge-data
        template: linkmerge
        arguments:
          parameters:
            - name: pvc-name
              value: '{{steps.generate-volume.outputs.parameters.pvc-name}}'

  - name: generate-volume
    inputs:
      parameters:
        - name: pvc-size
    resource:
      action: create
      setOwnerReference: true
      # Note for accessModes below. There were issues with using ReadWriteMany because the assigned
      # storage class was standard-rwo (read-write-once). 
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          generateName: pvc-example-
        spec:
          accessModes: ['ReadWriteOnce']
          resources:
            requests:
              storage: '{{inputs.parameters.pvc-size}}'
    outputs:
      parameters:
        - name: pvc-name
          valueFrom:
            jsonPath: '{.metadata.name}'
  
  - name: load-from-trino
    inputs:
      parameters:
        - name: pvc-name
    securityContext:
      # Following: https://stackoverflow.com/questions/50156124/kubernetes-nfs-persistent-volumes-permission-denied
      fsGroup: 2000 
    volumes:
      - name: data
        persistentVolumeClaim:
          claimName: '{{inputs.parameters.pvc-name}}'
    script:
      image: us-central1-docker.pkg.dev/neon-shared-service/neonscience/neon-is-data-src-trino:v1.5.0
      resources:
        requests:
          cpu: 1
          memory: 200M
      volumeMounts:                     # same syntax as k8s Pod spec
      - name: data
        mountPath: /mnt/vol
      command: [bash]
      source: |
        year="2025"
        month="01"
        day="10"
        export GEN_DATE=$year-$month-$day
        export GEN_OUTPUT_DIR="/mnt/vol/trino/$SOURCE_TYPE/$year/$month/$day"
        /usr/src/app/genscript/genparquet.py --storesitename --codec gzip --truncperiod second
      env:
      - name: LOG_LEVEL 
        value: INFO
      - name: SOURCE_TYPE
        value: 'ptb330a'
      - name: GEN_SITE_NAME 
        value: "HARV"
      - name: REQUESTS_CA_BUNDLE 
        value: "/etc/pki/tls/cert.pem"
      - name: GEN_YAML_CONF 
        value: "/usr/src/app/genscript/configs/ptb330a_streams.yaml"
      - name: GEN_SCHEMA_FILE 
        value: "/usr/src/app/schemas/ptb330a/ptb330a.avsc"
      - name: PRESTO_HOST  # name of env var
        valueFrom:
          secretKeyRef:
            name: trino-secret     # name of an existing k8s secret
            key: TRINO_HOST     # 'key' subcomponent of the secret    
      - name: PRESTO_USER  # name of env var
        valueFrom:
          secretKeyRef:
            name: trino-secret     # name of an existing k8s secret
            key: TRINO_USER     # 'key' subcomponent of the secret    
      - name: PRESTO_PASSWORD  # name of env var
        valueFrom:
          secretKeyRef:
            name: trino-secret     # name of an existing k8s secret
            key: TRINO_PASSWORD     # 'key' subcomponent of the secret    
    # outputs:
    #   artifacts:
    #   - name: trino_data
    #     path: /mnt/vol/trino
        
  - name: linkmerge
    serviceAccountName: argo
    inputs:
      parameters:
        - name: pvc-name
    securityContext:
      # Following: https://stackoverflow.com/questions/50156124/kubernetes-nfs-persistent-volumes-permission-denied
      fsGroup: 2000 
    volumes:
      - name: data
        persistentVolumeClaim:
          claimName: '{{inputs.parameters.pvc-name}}'
    script:
      image: us-central1-docker.pkg.dev/neon-shared-service/neonscience/neon-is-data-src-trino:v1.5.0
      volumeMounts:                     # same syntax as k8s Pod spec
      - name: data
        mountPath: /mnt/vol
      command: [bash]
      source: |-
        /bin/bash <<'EOF'
        
        # Use bash-scrict mode. See http://redsymbol.net/articles/unofficial-bash-strict-mode/
        set -euo pipefail
        IFS=$'\n\t'
        
        # Run second module - parquet_linkmerge (merges data from a source id that collected data from multiple sites in one day
        python3 -m parquet_linkmerge.parquet_linkmerge_main
        
        # Export L0 data to bucket
        if [[ -d "$OUT_PATH/$SOURCE_TYPE" ]]; then
          linkdir=$(mktemp -d)
          shopt -s globstar
          out_parquet_glob="${OUT_PATH}/**/*.parquet"
          # Example: /mnt/vol/out/li191r/2023/01/01/12345/data/file.parquet
          echo "Linking output files to ${linkdir}"
          set -x
          for f in $out_parquet_glob; do
            # Parse the path
            [[ "$f" =~ ^$OUT_PATH/(.*)/([0-9]+)/([0-9]+)/([0-9]+)/(.*)/data/(.*)$ ]]
            fsourcetype="${BASH_REMATCH[1]}"
            fyear="${BASH_REMATCH[2]}"
            fmonth="${BASH_REMATCH[3]}"
            fday="${BASH_REMATCH[4]}"
            fsourceid="${BASH_REMATCH[5]}"
            fname="${BASH_REMATCH[6]}"
            outdir="${linkdir}/v1/${fsourcetype}/ms=${fyear}-${fmonth}/source_id=${fsourceid}"
            mkdir -p "${outdir}"
            ln -s "${f}" "${outdir}/${fname}"
          done
          set +x
          echo "Syncing files to bucket"
          rclone \
            --no-check-dest \
            --copy-links \
            --gcs-bucket-policy-only \
            --gcs-no-check-bucket \
            copy \
            "${linkdir}" \
            ":gcs://${BUCKET_NAME}"
          echo "Removing temporary files"
          rm -rf $linkdir
        fi
        EOF
      env:
      - name: LOG_LEVEL 
        value: DEBUG
      - name: IN_PATH
        value: "/mnt/vol/trino"
      - name: OUT_PATH
        value: "/mnt/vol/out"
      - name: SOURCE_TYPE
        value: 'ptb330a'
      - name: SOURCE_TYPE_INDEX
        value: '3'
      - name: YEAR_INDEX
        value: '4'
      - name: MONTH_INDEX
        value: '5'
      - name: DAY_INDEX
        value: '6'
      - name: SOURCE_ID_INDEX
        value: '7'
      - name: BUCKET_NAME  # name of env var
        valueFrom:
          secretKeyRef:
            name: l0-bucket     # name of an existing k8s secret
            key: LO_BUCKET     # 'key' subcomponent of the secret    
    # inputs:
    #   artifacts:
    #   - name: trino_unmerged_data
    #     path: /mnt/vol/trino_unmerged_data
    outputs:
      artifacts:
      - name: linkmerge_data
        path: /mnt/vol/out
