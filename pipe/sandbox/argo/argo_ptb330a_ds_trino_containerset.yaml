---
### Using a containerset means no artifacts uploaded and downloaded between 
### steps for the linkmerge
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ptb330a-ds-trino-containerset-
spec:
  entrypoint: containerset-trino-example
  templates:
  # No output spec here because rclone is loading data right now
  - name: containerset-trino-example
    volumes:
    - name: workspace
      emptydir: {}
    - name: tmpdir
      emptydir: {}
    containerSet:
      retryStrategy:
        retries: 10  # If it fails, retry at most ten times
        duration: 30s  # Retry for at most 30s
      volumeMounts:
      - name: workspace
        mountPath: /mnt
      - name: tmpdir
        mountPath: /tmp
      containers:
      - name: main  # Loads from trino, called main in case we want to upload artifacts
        image: us-central1-docker.pkg.dev/neon-shared-service/neonscience/neon-is-data-src-trino:v1.5.0
        securityContext:
          runAsUser: 1001
        resources:
          requests:
            cpu: 1
            memory: 500M
          limits:
            cpu: 1.5
            memory: 1G
        command:
        - bash
        - -c
        - |
          whoami
          year="2025"
          echo $year
          month="01"
          day="10"
          GEN_DATE=$year-$month-$day \
          GEN_OUTPUT_DIR="/mnt/trino/$SOURCE_TYPE/$year/$month/$day" \
          /usr/src/app/genscript/genparquet.py --storesitename --codec gzip --truncperiod second
        env:
        - name: LOG_LEVEL
          value: INFO
        - name: SOURCE_TYPE
          value: 'ptb330a'
        - name: GEN_SITE_NAME
          value: "HARV"
        - name: REQUESTS_CA_BUNDLE
          value: "/etc/pki/tls/cert.pem"
        - name: GEN_YAML_CONF
          value: "/usr/src/app/genscript/configs/ptb330a_streams.yaml"
        - name: GEN_SCHEMA_FILE
          value: "/usr/src/app/schemas/ptb330a/ptb330a.avsc"
        - name: PRESTO_HOST  # name of env var
          valueFrom:
            secretKeyRef:
              name: trino-secret   # name of an existing k8s secret
              key: TRINO_HOST      # 'key' subcomponent of the secret
        - name: PRESTO_USER        # name of env var
          valueFrom:
            secretKeyRef:
              name: trino-secret   # name of an existing k8s secret
              key: TRINO_USER      # 'key' subcomponent of the secret
        - name: PRESTO_PASSWORD    # name of env var
          valueFrom:
            secretKeyRef:
              name: trino-secret   # name of an existing k8s secret
              key: TRINO_PASSWORD  # 'key' subcomponent of the secret
      - name: linkmerge
        dependencies: ["main"]
        image: us-central1-docker.pkg.dev/neon-shared-service/neonscience/neon-is-data-src-trino:v1.5.0
        securityContext:
          runAsUser: 1001
        # These get summed together in a containerset, and since this depends on the other
        # container it can just use the same resources
        # resources:
        #   requests:
        #     cpu: 1
        #     memory: 500M
        #   limits:
        #     cpu: 1.5
        #     memory: 1G
        command:
        - bash
        - -c
        - |
          # Use bash-scrict mode. See http://redsymbol.net/articles/unofficial-bash-strict-mode/
          set -euo pipefail
          IFS=$'\n\t'
          
          # Run second module - parquet_linkmerge (merges data from a source id 
          # that collected data from multiple sites in one day
          python3 -m parquet_linkmerge.parquet_linkmerge_main
          
          # Export L0 data to bucket
          if [[ -d "$OUT_PATH/$SOURCE_TYPE" ]]; then
            linkdir=$(mktemp -d)
            shopt -s globstar
            out_parquet_glob="${OUT_PATH}/**/*.parquet"
            # Example: /mnt/out/li191r/2023/01/01/12345/data/file.parquet
            echo "Linking output files to ${linkdir}"
            set -x
            for f in $out_parquet_glob; do
              # Parse the path
              [[ "$f" =~ ^$OUT_PATH/(.*)/([0-9]+)/([0-9]+)/([0-9]+)/(.*)/data/(.*)$ ]]
              fsourcetype="${BASH_REMATCH[1]}"
              fyear="${BASH_REMATCH[2]}"
              fmonth="${BASH_REMATCH[3]}"
              fday="${BASH_REMATCH[4]}"
              fsourceid="${BASH_REMATCH[5]}"
              fname="${BASH_REMATCH[6]}"
              outdir="${linkdir}/v1/${fsourcetype}/ms=${fyear}-${fmonth}/source_id=${fsourceid}"
              mkdir -p "${outdir}"
              ln -s "${f}" "${outdir}/${fname}"
            done
            set +x
            echo "Syncing files to bucket"
            rclone \
              --no-check-dest \
              --copy-links \
              --gcs-bucket-policy-only \
              --gcs-no-check-bucket \
              copy \
              "${linkdir}" \
              ":gcs://${BUCKET_NAME}"
            echo "Removing temporary files"
            rm -rf $linkdir
          fi
        env:
        - name: LOG_LEVEL
          value: DEBUG
        - name: IN_PATH
          value: "/mnt/trino_unmerged_data"
        - name: OUT_PATH
          value: "/mnt/out"
        - name: SOURCE_TYPE
          value: 'ptb330a'
        - name: SOURCE_TYPE_INDEX
          value: '3'
        - name: YEAR_INDEX
          value: '4'
        - name: MONTH_INDEX
          value: '5'
        - name: DAY_INDEX
          value: '6'
        - name: SOURCE_ID_INDEX
          value: '7'
        - name: BUCKET_NAME  # name of env var
          value: neon-nonprod-argo-artifacts
