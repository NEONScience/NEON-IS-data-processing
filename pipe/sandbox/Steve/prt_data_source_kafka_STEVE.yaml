---
pipeline:
  name: prt_data_source_kafka_STEVE
transform:
  image: quay.io/battelleecology/kafka-loader:v1.0.6-kafka
  image_pull_secrets:
  - battelleecology-quay-read-all-pull-secret
  cmd: ["/bin/bash"]
  env:
    OUT_PATH: /pfs/out
    SOURCE_TYPE: "prt"
    LOGLEVEL: debug
    YEAR_INDEX: "5"
    MONTH_INDEX: "6"
    DAY_INDEX: "7"
    KAFKA_RETENTION_DAYS: "30"
  secrets:
  - name: pachyderm-kafka-auth
    env_var: KAFKA_USER
    key: KAFKA_USER
  - name: pachyderm-kafka-auth
    env_var: KAFKA_PASSWORD
    key: KAFKA_PASSWORD
  - name: pachyderm-kafka-auth
    env_var: KAFKA_BROKER
    key: KAFKA_BROKER
  stdin:
  - "#!/bin/bash"
  - "# Get GCP zone"
  - "meta=$(curl -sH 'Metadata-Flavor: Google' http://metadata/computeMetadata/v1/instance/zone)"
  - zone=$(echo $meta | cut -d "/" -f 4)
  - echo $zone
  - "# Get today's date for evaluating kafka data retention period"
  - date_today=$(date -u +%Y-%m-%d)
  - kafka_min_date=$(date -u -d "$KAFKA_RETENTION_DAYS days ago" +%Y-%m-%d)
  - "# Get date from input path. Terminal path structure must be /SOURCE_TYPE/YYYY/MM/DD/SITE_FILE"
  - "# Datum must be set at /SOURCE_TYPE/YYYY/MM/DD or /SOURCE_TYPE/YYYY/MM/DD/SITE_FILE"
  - date_path=$(echo $import_trigger | cut -f $YEAR_INDEX,$MONTH_INDEX,$DAY_INDEX -d "/")
  - echo $date_path
  - date_str=$(date -u +%Y-%m-%d -d $date_path)
  - "# Get each site to run"
  - for site_kafka in $(dir ${import_trigger[*]}); do
  - site_file=$(basename $site_kafka) # Strip off any path prefix
  - site=$(echo $site_file | cut -f 1 -d "." --only-delimited) # Extract the site from site.kafka. Ignore site-only files (e.g. CPER vs. CPER.kafka)
  - type=$(echo $site_file | cut -f 2 -d "." --only-delimited) # Extract the 'kafka' from site.kafka
  - if [ "$type" != "kafka" ]
  - then 
      echo "$site_file is not indicated to be streaming from Kafka. Skipping...";
  - elif [ $(date -u +%s -d $date_str) -lt $(date -u +%s -d $kafka_min_date) ]
  - then
      echo "Cannot extract $date_str Kafka data for $site. Today's date ($date_today) is beyond the Kafka retention period ($KAFKA_RETENTION_DAYS days). Skipping...";
  - else
      echo "Extracting $date_str kafka data for $site";
      ./extract-kafka-sensor.py -s $site -S $SOURCE_TYPE -D $OUT_PATH/$SOURCE_TYPE -d $date_str --only current --consumer client.rack=$zone;
      date_str_1=$(date +%Y-%m-%d -d "$date_str + 1 day");
      ./extract-kafka-sensor.py -s $site -S $SOURCE_TYPE -D $OUT_PATH/$SOURCE_TYPE -d $date_str_1 --only noncurrent --consumer client.rack=$zone
  - fi
  - done
input:
  pfs:
    name: import_trigger
    repo: prt_cron_daily_and_date_control_STEVE
    # Must be datum by day (e.g. /SOURCE_TYPE/*/*/*) or by day/site (e.g. /SOURCE_TYPE/*/*/*/*)
    glob: "/prt/*/*/*"
resource_requests:
  memory: 500M
  cpu: 1.3
parallelism_spec:
  constant: 10
autoscaling: true
scheduling_spec:
  node_selector:
    cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
    nodepool.neonscience.org/pipeline: "yes"
    cloud.google.com/gke-spot: "true"
pod_spec: |-
  { "tolerations": [
    {
      "key": "nodepool.neonscience.org/pipeline",
      "operator": "Exists"
    },
    {
      "effect": "NoSchedule",
      "key": "cloud.google.com/gke-spot",
      "operator": "Exists"
    }  
  ] }
pod_patch: |-
  [
    { "op": "replace",
      "path":"/containers/1/resources/requests/memory",
      "value":"3G"
    },
    { "op": "replace",
      "path": "/containers/1/resources/requests/cpu",
      "value": "0.3"
    }
  ]
