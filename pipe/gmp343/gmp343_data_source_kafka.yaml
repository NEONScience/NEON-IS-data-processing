---
pipeline:
  name: gmp343_data_source_kafka
transform:
  image: us-central1-docker.pkg.dev/neon-shared-service/bei/neon-avro-kafka-loader:v4.11.0
  image_pull_secrets:
  - battelleecology-quay-read-all-pull-secret
  env:
    OUT_PATH: /pfs/out
    SOURCE_TYPE: "gmp343"
    LOG_LEVEL: INFO
    YEAR_INDEX: "5"
    MONTH_INDEX: "6"
    DAY_INDEX: "7"
    KAFKA_RETENTION_DAYS: "15"
  secrets:
  - name: pachyderm-kafka-auth
    env_var: KAFKA_USER
    key: KAFKA_USER
  - name: pachyderm-kafka-auth
    env_var: KAFKA_PASSWORD
    key: KAFKA_PASSWORD
  - name: pachyderm-kafka-auth
    env_var: KAFKA_BROKER
    key: KAFKA_BROKER
  - name: pachyderm-kafka-auth
    env_var: KAFKA_LOG_TOPIC
    key: KAFKA_LOG_TOPIC
  - name: l0-bucket
    env_var: BUCKET_NAME
    key: LO_BUCKET
  - name: pdr-secret
    env_var: PDR_HOST
    key: hostname
  - name: pdr-secret
    env_var: PDR_DBNAME
    key: database
  - name: pdr-secret
    env_var: PDR_USER
    key: username
  - name: pdr-secret
    env_var: PDR_PASSWORD
    key: password
  cmd:
  - sh
  - "-c"
  - |-
    /bin/bash <<'EOF'
    # Use bash-scrict mode. See http://redsymbol.net/articles/unofficial-bash-strict-mode/
    set -euo pipefail
    IFS=$'\n\t'

    # Get GCP zone
    meta=$(curl -sH "Metadata-Flavor: Google" "http://metadata/computeMetadata/v1/instance/zone")
    zone=$(echo "${meta}" | cut -d "/" -f 4)
    echo "GCP Zone: ${zone}"

    # Get today's date for evaluating kafka data retention period
    date_today=$(date -u +%Y-%m-%d)
    kafka_min_date=$(date -u -d "${KAFKA_RETENTION_DAYS} days ago" +%Y-%m-%d)

    # Get date from input path. Terminal path structure must be /SOURCE_TYPE/YYYY/MM/DD/SITE_FILE
    # Datum must be set at /SOURCE_TYPE/YYYY/MM/DD or /SOURCE_TYPE/YYYY/MM/DD/SITE_FILE
    date_path=$(echo "${import_trigger}" | cut -f "${YEAR_INDEX},${MONTH_INDEX},${DAY_INDEX}" -d "/")
    echo "${date_path}"
    date_str=$(date -u +%Y-%m-%d -d "${date_path}")

    # Get each site to run
    if [[ -f ${import_trigger} ]]; then
      import_trigger_glob="${import_trigger}"
    else
      import_trigger_glob="${import_trigger}/*"
    fi

    mkdir -p "${OUT_PATH}/${SOURCE_TYPE}"

    for site_kafka in ${import_trigger_glob}; do
      site_file=$(basename "${site_kafka}") # Strip off any path prefix
      site=$(echo "${site_file}" | cut -f 1 -d "." --only-delimited) # Extract the site from site.kafka. Ignore site-only files (e.g. CPER vs. CPER.kafka)
      type=$(echo "${site_file}" | cut -f 2 -d "." --only-delimited) # Extract the 'kafka' from site.kafka
      if [[ "${type}" != "kafka" ]]
      then
        echo "${site_file} is not indicated to be streaming from Kafka. Skipping..."
        continue
      elif [[ "$(date -u +%s -d "${date_str}")" -lt "$(date -u +%s -d "${kafka_min_date}")" ]]
      then
        echo -n "Cannot extract ${date_str} Kafka data for ${site}. "
        echo -n "Today's date (${date_today}) is beyond the Kafka retention period (${KAFKA_RETENTION_DAYS} days). Skipping..."
        continue
      fi
      
      # We are ok to run
      echo "Extracting ${date_str} kafka data for ${site}"
      
      # Make a directory to store the output files by site
      siteoutpath="${OUT_PATH}/${site}"
      mkdir -p "${siteoutpath}"

      # Get "current data" - data that came in on the specified day, which is the same day it was measured
      # Note: We cannot use the --removeoffset flag on the kafka loader (which removes the offsets from the filenames. This will often violate the Pachyderm requirement that different datums cannot write the same file)
      ./extract-kafka-sensor.py -s "${site}" -S "${SOURCE_TYPE}" -D "${siteoutpath}/${SOURCE_TYPE}" -d "${date_str}" --only current --consumer "client.rack=${zone}"
      
      # Get "non-current data" - data that came in on the specified day, which is NOT the same day it was measured
      date_str_1=$(date +%Y-%m-%d -d "${date_str} + 1 day")
      ./extract-kafka-sensor.py -s "${site}" -S "${SOURCE_TYPE}" -D "${siteoutpath}/${SOURCE_TYPE}" -d "${date_str_1}" --only noncurrent --consumer client.rack="${zone}"

      # Upload L0 files to bucket, compacting with any existing file with the same name
      if [[ -d "${siteoutpath}/${SOURCE_TYPE}" ]]; then
        linkdir=$(mktemp -d)
        shopt -s globstar
        out_parquet_glob="${siteoutpath}/**/*.parquet"
        # /pfs/out/ABBY/gmp343/2023/01/01/12345/data/file.parquet
        echo "Linking output files to ${linkdir}"
        # set -x # Uncomment for debugging
        for f in ${out_parquet_glob}; do
          # Parse the path
          [[ "${f}" =~ ^${siteoutpath}/(.*)/([0-9]+)/([0-9]+)/([0-9]+)/(.*)/data/(.*)$ ]]
          fsourcetype="${BASH_REMATCH[1]}"
          fyear="${BASH_REMATCH[2]}"
          fmonth="${BASH_REMATCH[3]}"
          fday="${BASH_REMATCH[4]}"
          fsourceid="${BASH_REMATCH[5]}"
          fname="${BASH_REMATCH[6]}"
          # fname_out="${fsourcetype}_${fsourceid}_${fyear}-${fmonth}-${fday}.parquet"  # Remove offsets from the filename
          outdir="${linkdir}/v2/${fsourcetype}/ms=${fyear}-${fmonth}/source_id=${fsourceid}"
          mkdir -p "${outdir}"
          ln -s "${f}" "${outdir}/${fname}"
        done

        # Upload to bucket, compacting with any existing file
        echo "Uploading and compacting for ${site} ${date_str}"
        ./compact-bucket-copy.py --sourcepath "${linkdir}" --destbucket "${BUCKET_NAME}" --stripoffset

        # Update the airflow triggering table
        echo "Updating airflow trigger table for ${site} ${date_str}"
        ./update-trigger-table.py -s "${site}" -S "${SOURCE_TYPE}" -D "${siteoutpath}/${SOURCE_TYPE}"

        # set +x # Uncomment for debugging
        rm -rf "${linkdir}"
        # Move the per site files to the structure expected by pachyderm
        cd "${siteoutpath}/${SOURCE_TYPE}/"
        dest="${OUT_PATH}/${SOURCE_TYPE}/"
        find . -type f | while read f; do
            d=$(dirname "$f")
            file=$(basename "$f")
            mkdir -p "$dest/$d"
            mv -f "$f" "$dest/$d/"
        done
        cd -
        # Remove the site directory
        rm -rf "${siteoutpath}"
      fi

    done
    EOF
input:
  pfs:
    name: import_trigger
    repo: gmp343_cron_daily_and_date_control
    # Must be datum by day (e.g. /SOURCE_TYPE/*/*/*) or by day/site (e.g. /SOURCE_TYPE/*/*/*/*)
    glob: "/gmp343/*/*/*"
parallelism_spec:
  constant: 3
autoscaling: true
resource_requests:
  memory: 300M
  cpu: 1.8
resource_limits:
  memory: 1.5G
  cpu: 2.5
sidecar_resource_requests:
  memory: 1.5G
  cpu: 0.5
datum_set_spec:
  number: 1
scheduling_spec:
  node_selector:
    cloud.google.com/compute-class: pach-pipeline-class

